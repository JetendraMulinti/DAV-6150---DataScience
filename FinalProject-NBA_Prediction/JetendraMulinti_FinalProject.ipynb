{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef26a65-6652-49ab-af9d-9b7f70eea062",
   "metadata": {},
   "source": [
    "## Abstract:\n",
    "This project aims to predict NBA game outcomes using machine learning techniques. We collect and preprocess a comprehensive dataset spanning multiple seasons, including game statistics and player performance metrics. Through exploratory analysis and feature engineering, we identify the most influential factors in determining game results. We develop and evaluate various models, such as logistic regression, support vector machines, and XGBoost, using cross-validation and time-series splitting. Our approach incorporates SHAP for model interpretability and understanding feature importance. The ultimate goal is to provide accurate predictions and valuable insights to support decision-making and strategy in the NBA. The project's findings have implications for coaches, managers, analysts, and fans, offering a data-driven perspective on the factors that drive success in professional basketball."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2e55c7-5abe-477e-a0d0-1e3b19f3f590",
   "metadata": {},
   "source": [
    "## Introdction:  \n",
    "\n",
    "In this project, we leverage machine learning techniques to predict NBA game outcomes. Our goal is to develop an accurate and robust model that can provide valuable insights for coaches, managers, and analysts. By harnessing historical game data and advanced analytics, we aim to answer key questions such as the impact of statistical indicators, in-season changes, and home-court advantage on game results. Through rigorous data preprocessing, feature engineering, and model evaluation, we strive to create a tool that can revolutionize decision-making and strategy in the NBA. Join us as we explore the power of data science in predicting the outcomes of professional basketball games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7644f5bd-fac5-46c2-a05d-32e57253c42d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutliers_influence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variance_inflation_factor\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "### Libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, StratifiedKFold, cross_val_score, learning_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import shap\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26be90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of GitHub raw URLs\n",
    "urls = [\n",
    "    'https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2020.csv',\n",
    "    'https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2021.csv',\n",
    "    'https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2022.csv',\n",
    "    'https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2023.csv'\n",
    "]\n",
    "\n",
    "all_cleaned_dataframes = []\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Load the dataframe from pickle data obtained from URL\n",
    "        dataframe = pd.read_csv(url)\n",
    "        \n",
    "        # Filter out columns that start with 'Unnamed:'\n",
    "        dataframe = dataframe.loc[:, ~dataframe.columns.str.startswith('Unnamed:')]\n",
    "\n",
    "        # Drop all columns that are entirely NA\n",
    "        dataframe = dataframe.dropna(axis=1, how='all')\n",
    "\n",
    "        # Add the cleaned dataframe to the list\n",
    "        all_cleaned_dataframes.append(dataframe)\n",
    "        \n",
    "        print(f\"Processed data from {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "df = pd.concat(all_cleaned_dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "### delete some more columns\n",
    "columns_to_delete = ['OT', 'OT_opp', '2OT', '3OT', '2OT_opp', '3OT_opp',\n",
    "                     ## '4OT', '4OT_opp',\n",
    "                    'mp_total_opp','bpm_max','bpm_max_opp']\n",
    "\n",
    "# Drop the specified columns from the dataframe\n",
    "df.drop(columns=columns_to_delete, inplace=True)\n",
    "\n",
    "\n",
    "print(\"No of duplicate rows: \",df.duplicated().sum())\n",
    "\n",
    "### Drop duplicates\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(\"No of duplicate rows after dropping duplicates: \",df.duplicated().sum())\n",
    "\n",
    "#### rename columns\n",
    "df.rename(columns = {'mp_total':'mp'}, inplace=True)\n",
    "\n",
    "#### Creating Season column\n",
    "df['date'] = pd.to_datetime(df['date'])  # Convert 'date' column to datetime if it's not already\n",
    "\n",
    "# Function to determine the season year based on the month\n",
    "def get_season_year(row):\n",
    "    if row['date'].month >= 10:\n",
    "        return row['date'].year\n",
    "    else:\n",
    "        return row['date'].year - 1\n",
    "\n",
    "# Apply the function to create a new 'season' column\n",
    "df['season'] = df.apply(get_season_year, axis=1)\n",
    "\n",
    "\n",
    "print(\"data shape:\", df.shape)\n",
    "\n",
    "columns_format = list(df.columns)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b08474-161c-487f-9bd0-ce1533ffa56e",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083779e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Abbrivate the Team names\n",
    "team_df = pd.read_csv('https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/Team_full-forms.csv')\n",
    "team_df['team'] = team_df['team'].str.strip()\n",
    "team_df['team1'] = team_df['team1'].str.strip()\n",
    "\n",
    "\n",
    "##### Merge and delete the columns\n",
    "df = pd.merge(team_df, df, on = ['team'], how='inner')\n",
    "del df['team']\n",
    "df.rename(columns = {'team1':'team'}, inplace=True)\n",
    "\n",
    "team_df.rename(columns = {'team':'team_opp'}, inplace=True)\n",
    "df = pd.merge(team_df, df, on = ['team_opp'], how='inner')\n",
    "del df['team_opp']\n",
    "df.rename(columns = {'team1':'team_opp'}, inplace=True)\n",
    "\n",
    "print(\"data shape:\", df.shape)\n",
    "\n",
    "df = df[columns_format]\n",
    "\n",
    "## ordering with date\n",
    "df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "df = df.sort_values(by = ['date'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4530f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68386cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### checking the data is balance / Imbalanced\n",
    "\n",
    "df['won'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded602c",
   "metadata": {},
   "source": [
    "Checking Null values and dropping columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07971fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checking null values\n",
    "\n",
    "null_columns = df.isnull().sum()\n",
    "null_columns[null_columns > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### delete some more columns\n",
    "more_columns_to_delete = ['index_opp']\n",
    "\n",
    "# Drop the specified columns from the dataframe\n",
    "df.drop(columns=more_columns_to_delete, inplace=True)\n",
    "\n",
    "## as we have only 1 null row (match) we will drop it\n",
    "df = df.dropna()\n",
    "\n",
    "null_columns = df.isnull().sum()\n",
    "null_columns[null_columns > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c70da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## re-ordering on date\n",
    "\n",
    "## ordering with date\n",
    "df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "df = df.sort_values(by = ['date'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "print(\"data shape:\", df.shape)\n",
    "\n",
    "\n",
    "print(\"no of rows before: \", len(df))\n",
    "\n",
    "#### Deleting the repeated rows (Instead of both perpestives)\n",
    "# Create a sorted string that combines team names and game date\n",
    "df['game_id'] = df.apply(lambda x: '_'.join(sorted([x['team'], x['team_opp']])) + '_' + str(x['date']), axis=1)\n",
    "\n",
    "# Keep only one entry per game based on the alphabetical order of team names\n",
    "df = df.sort_values(by=['team', 'team_opp']).drop_duplicates(subset='game_id', keep='first').reset_index(drop=True)\n",
    "\n",
    "print(\"no of rows After: \", len(df))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb46998-65b5-4379-b396-90ff490b9736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0bd509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20498b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec59ebfc-3814-405a-8cef-29e9cd63ab3d",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b7d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for key metrics\n",
    "key_metrics = ['fg_total', 'fga_total', 'fg%_total', '3p_total', '3pa_total', '3p%_total', 'ft_total',\n",
    "               'fta_total', 'ft%_total', 'total_opp', '+/-_max']\n",
    "\n",
    "# Selecting the key metrics and generating descriptive statistics\n",
    "key_stats_summary = df[key_metrics].describe()\n",
    "\n",
    "# Display the descriptive statistics for key metrics\n",
    "key_stats_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd0c9bd-fc2f-4742-be78-61318919eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for key metrics\n",
    "key_metrics_opp = ['fg_total_opp', 'fga_total_opp', 'fg%_total_opp', '3p_total_opp', '3pa_total_opp', '3p%_total_opp', 'ft_total_opp',\n",
    "               'fta_total_opp', 'ft%_total_opp', '+/-_max_opp']\n",
    "\n",
    "# Selecting the key metrics and generating descriptive statistics\n",
    "key_stats_summary_opp = df[key_metrics_opp].describe()\n",
    "\n",
    "# Display the descriptive statistics for key metrics\n",
    "key_stats_summary_opp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e9cde-0e3f-4785-bbf7-96d17420aad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "800bcae8",
   "metadata": {},
   "source": [
    "1. Field Goals Made and Attempted (fg_total, fga_total): Teams make an average of 40 field goals per game from 87 attempts, translating to an average field goal percentage of 46.2%.\n",
    "2. Three-Point Shots (3p_total, 3pa_total, 3p%_total): On average, teams successfully make 11 three-point shots per game from 31 attempts, achieving a three-point shooting percentage of 35.7%.\n",
    "3. Free Throws (ft_total, fta_total, ft%_total): Teams typically make 17 free throws per game from 23 attempts, with an average success rate of 77.2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f6625",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 20))  # Adjust the subplot grid to 4x3\n",
    "\n",
    "# Plotting field goals, three-point shots, and free throws\n",
    "sns.histplot(df['fg_total'], bins=30, kde=True, ax=axes[0, 0]).set_title('Field Goals Made')\n",
    "sns.histplot(df['3p_total'], bins=30, kde=True, ax=axes[0, 1]).set_title('Three-Points Made')\n",
    "sns.histplot(df['ft_total'], bins=30, kde=True, ax=axes[0, 2]).set_title('Free Throws Made')\n",
    "\n",
    "# Plotting percentages for field goals, three-point shots, and free throws\n",
    "sns.histplot(df['fg%_total'], bins=30, kde=True, ax=axes[1, 0]).set_title('Field Goal Percentage')\n",
    "sns.histplot(df['3p%_total'], bins=30, kde=True, ax=axes[1, 1]).set_title('Three-Point Percentage')\n",
    "sns.histplot(df['ft%_total'], bins=30, kde=True, ax=axes[1, 2]).set_title('Free Throw Percentage')\n",
    "\n",
    "# Plotting field goals, three-point shots, and free throws made by opponents\n",
    "sns.histplot(df['fg_total_opp'], bins=30, kde=True, ax=axes[2, 0]).set_title('Field Goals Made by Opp')\n",
    "sns.histplot(df['3p_total_opp'], bins=30, kde=True, ax=axes[2, 1]).set_title('Three-Points Made by Opp')\n",
    "sns.histplot(df['ft_total_opp'], bins=30, kde=True, ax=axes[2, 2]).set_title('Free Throws Made by Opp')\n",
    "\n",
    "# Plotting percentages for field goals, three-point shots, and free throws made by opponents\n",
    "sns.histplot(df['fg%_total_opp'], bins=30, kde=True, ax=axes[3, 0]).set_title('Field Goal Percentage by Opp')\n",
    "sns.histplot(df['3p%_total_opp'], bins=30, kde=True, ax=axes[3, 1]).set_title('Three-Point Percentage by Opp')\n",
    "sns.histplot(df['ft%_total_opp'], bins=30, kde=True, ax=axes[3, 2]).set_title('Free Throw Percentage by Opp')\n",
    "\n",
    "# Plotting games per season and distributions of win and next game outcomes\n",
    "sns.countplot(x='season', data=df, ax=axes[4, 0]).set_title('Games per Season')\n",
    "sns.countplot(x='won', data=df, ax=axes[4, 1]).set_title('Win Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2badae-c404-4518-aef1-73e200e4a40f",
   "metadata": {},
   "source": [
    "1. Field Goals Made and Three-Points Made distributions center around a common range, indicating a pattern in scoring strategies across games.\n",
    "2. Percentage metrics for Field Goals, Three-Points, and Free Throws exhibit a normal distribution, reflecting a standard level of efficiency across matches.\n",
    "3. The Games per Season distribution shows consistency in the number of games played, which supports analyses over multiple seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783381d1-d413-45c4-b1db-3657936504da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_trend(column):\n",
    "    # Check if the column data looks like percentages (values between 0 and 1)\n",
    "    if df[column].max() <= 1:\n",
    "        # If so, convert to percentage by multiplying by 100\n",
    "        seasonal_averages = df.groupby('season')[column].mean() * 100\n",
    "        ylabel = f'Average {column} (%)'\n",
    "    else:\n",
    "        # Otherwise, use the values as is\n",
    "        seasonal_averages = df.groupby('season')[column].mean()\n",
    "        ylabel = f'Average {column}'\n",
    "    \n",
    "    # Plotting the time series\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    seasonal_averages.plot(kind='line', marker='o')\n",
    "    plt.title(f'Average {column} by NBA Season')\n",
    "    plt.xlabel('NBA Season')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    plt.xticks(ticks=seasonal_averages.index, labels=seasonal_averages.index)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed311a65-ea3e-4fa2-bd05-bb2dda415118",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Field goal percentage (field goals made divided by field goal attempts)\n",
    "season_trend('fg%_total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf10d37-ee30-4c57-84e3-2127779e15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A statistic that measures the point differential when a player or team is on the court, indicating the impact on the game's score; a positive value means the team outscored opponents, while a negative value indicates being outscored.\n",
    "\n",
    "season_trend('+/-_max_opp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af47422-0066-4100-b9e2-3d07b3076566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6436cf0-7499-4b9d-9470-659e6aa9bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_team_performance( metric):\n",
    "    # Determine if the metric is a percentage (between 0 and 1)\n",
    "    percentage_scale = df[metric].max() <= 1\n",
    "    \n",
    "    # Calculate the average metric for each team per season\n",
    "    seasonal_team_averages = df.groupby(['season', 'team'])[metric].mean().unstack()\n",
    "\n",
    "    # Scale up if the metric is a percentage\n",
    "    if percentage_scale:\n",
    "        seasonal_team_averages *= 100\n",
    "        ylabel = f'Average {metric} (%)'\n",
    "    else:\n",
    "        ylabel = f'Average {metric}'\n",
    "\n",
    "    # Identify the top and bottom 5 performing teams\n",
    "    top_teams = seasonal_team_averages.mean(axis=0).sort_values(ascending=False).head(5).index\n",
    "    bottom_teams = seasonal_team_averages.mean(axis=0).sort_values(ascending=True).head(5).index\n",
    "\n",
    "    # Create subplots for the top and bottom performing teams\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "    # Top 5 performing teams plot\n",
    "    for team in top_teams:\n",
    "        axs[0].plot(seasonal_team_averages.index, seasonal_team_averages.loc[:, team], marker='o', label=team)\n",
    "    axs[0].set_title(f'Top 5 Performing Teams by {metric}')\n",
    "    axs[0].set_ylabel(ylabel)\n",
    "    axs[0].grid(True)\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Bottom 5 performing teams plot\n",
    "    for team in bottom_teams:\n",
    "        axs[1].plot(seasonal_team_averages.index, seasonal_team_averages.loc[:, team], marker='o', label=team)\n",
    "    axs[1].set_title(f'Bottom 5 Performing Teams by {metric}')\n",
    "    axs[1].set_ylabel(ylabel)\n",
    "    axs[1].grid(True)\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Set common X label\n",
    "    plt.xlabel('NBA Season')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671cc75d-fcf6-4e32-a852-1445c9e5dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_team_performance('fg%_total') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6991dd0-c2c8-4568-bb95-15cd1f1832bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_team_performance('+/-_max_opp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144e67f5-0bba-4a68-a5cf-5d37fd5bdda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# List of column names correctly passed to the correlation matrix plotting\n",
    "columns_to_include = [\n",
    " 'mp', 'fg_total', 'fga_total', 'fg%_total', '3p_total', '3pa_total', '3p%_total',\n",
    " 'ft_total', 'fta_total', 'ft%_total', 'orb_total', 'drb_total', 'trb_total',\n",
    " 'ast_total', 'stl_total', 'blk_total', 'tov_total', 'pf_total', 'pts_total',\n",
    " 'ts%_total', 'efg%_total', '3par_total', 'ftr_total', 'orb%_total', 'drb%_total',\n",
    " 'trb%_total', 'ast%_total', 'stl%_total', 'blk%_total', 'tov%_total',\n",
    " 'usg%_total', 'ortg_total', 'drtg_total', 'home', 'won'  # Including only relevant columns\n",
    "]\n",
    "\n",
    "# Ensure that all these columns exist in df before using them\n",
    "if set(columns_to_include).issubset(df.columns):\n",
    "    fig, ax = plt.subplots(figsize=(25, 25))\n",
    "    correlation_matrix = df[columns_to_include].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', ax=ax)\n",
    "    ax.set_title('Correlation Matrix of Selected Metrics with Target')\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plt.savefig('CorrelationMatrix.png')\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Some columns are missing in the DataFrame. Please check the column names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b92b45-bb2e-4377-85a3-0cdb74de1d1f",
   "metadata": {},
   "source": [
    "Multicollinearity Check Using Variance Inflation Factor (VIF)\n",
    "Purpose: Assessing multicollinearity among predictive features to ensure that the model is not unduly influenced by highly correlated independent variables. This helps in refining the model to improve prediction accuracy and reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfaa19e-6938-42a3-a139-17d19717bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Function to calculate VIF for each feature and provide specific suggestions\n",
    "def calculate_vif(data):\n",
    "    vif_df = pd.DataFrame()\n",
    "    vif_df[\"variables\"] = data.columns\n",
    "    vif_df[\"VIF\"] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n",
    "    \n",
    "    # Defining suggestions based on VIF values\n",
    "    def vif_suggestions(vif_value):\n",
    "        if vif_value <= 1:\n",
    "            return \"Keep: Not correlated\"\n",
    "        elif 1 < vif_value < 5:\n",
    "            return \"Keep: Moderately correlated\"\n",
    "        elif 5 <= vif_value < 10:\n",
    "            return \"Consider reviewing: Highly correlated\"\n",
    "        else:\n",
    "            return \"Remove or transform: Very high correlation\"\n",
    "\n",
    "    vif_df['Suggestion'] = vif_df['VIF'].apply(vif_suggestions)\n",
    "    return vif_df\n",
    "\n",
    "# Selecting numeric features for VIF calculation\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "vif_data = calculate_vif(df[numeric_cols].dropna())\n",
    "\n",
    "# Display VIF scores\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False).reset_index(drop=True)\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f02e98f-0152-45e9-bff7-ebdc8d0cfc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_data['Suggestion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b8913e-c3c2-49de-9567-51a3cba153fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f20375-bd3d-422f-b7f8-36624aa7ab1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ed04525-45e6-4d05-8200-f812d5a28693",
   "metadata": {},
   "source": [
    "Lagged Features for Dynamic Team Performance\n",
    "Purpose: Creating lagged features to assess how past game performances (e.g., points scored, rebounds) influence the outcome of future games. This analysis helps in understanding team momentum or fatigue, which can be crucial for predicting outcomes of future games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af036e4-1fda-4d8e-ad11-d1377b70edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the rolling windows you want to test\n",
    "rolling_windows = [1, 3, 5, 7, 10]\n",
    "\n",
    "# Assuming 'df' and 'pts_total' are already defined in your DataFrame\n",
    "# Create lagged features for each window size\n",
    "for window in rolling_windows:\n",
    "    # Create the lagged data for points\n",
    "    df[f'pts_scored_lag{window}'] = df.groupby('team')['pts_total'].shift(1).rolling(window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "\n",
    "# Drop rows with NaN values in the target column, which will appear at the end of each team's data\n",
    "df.dropna(subset=['won'], inplace=True)\n",
    "\n",
    "# Prepare the figure layout\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(14, 18))\n",
    "axes = axes.flatten()  # Flatten the array of axes for easier iteration\n",
    "\n",
    "# Plot the impact of each lagged feature on the target\n",
    "for i, window in enumerate(rolling_windows):\n",
    "    sns.boxplot(x='won', y=f'pts_scored_lag{window}', data=df, ax=axes[i])\n",
    "    axes[i].set_title(f'Impact of Average Points Last {window} Games Current Game Winning')\n",
    "    axes[i].set_xlabel('Game Won')\n",
    "    axes[i].set_ylabel(f'Points Scored in Last {window} Games')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig('Impact_of_Lagged_Features_on_Winning.png')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc21969-1674-461b-83c7-e703a7c82e8a",
   "metadata": {},
   "source": [
    "From the visual inspection, Lag 3 & Lag 1(average points from the last 3 games & last game) seems to provide the best balance between capturing enough historical performance to predict future outcomes and not including too much past data which dilutes the predictive power. The slight increase in median points for games that were won suggests that averaging over three games strikes a good balance in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eefcd62-24be-4046-b6f9-1a8e9ccf73fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff7aaa-4f61-4c14-a58d-891f43340ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plotting the impact of home-court advantage\n",
    "plt.figure(figsize=(6, 4))\n",
    "ax = sns.barplot(x='home', y='won', data=df, ci=None)  # ci=None to remove the confidence interval bars\n",
    "plt.title(\"Impact of Home Court on Winning\")\n",
    "plt.xlabel(\"Home Game (1 = Home, 0 = Away)\")\n",
    "plt.ylabel(\"Probability of Winning\")\n",
    "\n",
    "# Calculate the mean probabilities for annotations\n",
    "home_winning_probabilities = df.groupby('home')['won'].mean()\n",
    "\n",
    "# Annotate the bars with the calculated probabilities\n",
    "for i, p in enumerate(ax.patches):  # access the bars\n",
    "    ax.annotate(format(home_winning_probabilities.iloc[i], '.2f'),  # format the probability\n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()),  # position for the text\n",
    "                ha = 'center', va = 'center',  # center alignment\n",
    "                xytext = (0, 9),  # position text slightly above the bar\n",
    "                textcoords = 'offset points')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff0cecc-fe07-4f46-999c-de80fc782615",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b1d3d-fe54-4c50-a990-106397f2a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns = df.isnull().sum()\n",
    "null_columns[null_columns > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97207551-3a23-4270-be9b-47fb8dd2ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "### delete null columns\n",
    "\n",
    "try:\n",
    "    df.drop(columns=['pts_scored_lag1','pts_scored_lag3','pts_scored_lag5',\n",
    "                    'pts_scored_lag7','pts_scored_lag10'], inplace=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec235a0-c0e5-49c0-9da9-f40cb8260379",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns = df.isnull().sum()\n",
    "null_columns[null_columns > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc2809d-a66c-45d1-9b69-8889ddb6eb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34fb0859-2290-439e-97ab-fe1c67cdf74f",
   "metadata": {},
   "source": [
    "Checking whether the target column is repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff338f2-ea22-41d4-9654-93569ba53ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'won'  \n",
    "\n",
    "# Find any columns that have the same values as the target column\n",
    "similar_columns = []\n",
    "for column in df.columns:\n",
    "    if column != target_column and df[column].equals(df[target_column]):\n",
    "        similar_columns.append(column)\n",
    "\n",
    "if similar_columns:\n",
    "    print(\"Columns with identical values to the target column '{}':\".format(target_column), similar_columns)\n",
    "else:\n",
    "    print(\"No columns have identical values to the target column '{}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf73643e-5f3b-4545-8dcb-5d26cf55874d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbdaa87-5c20-48ce-9052-fc14cdce853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['season'] = df['season'].astype(str)  # Convert 'season' to string type\n",
    "\n",
    "# Select columns of data type 'category', 'object', and 'bool'\n",
    "categorical_bool_columns = df.select_dtypes(include=['category', 'object', 'bool']).columns\n",
    "\n",
    "# Convert to list and remove 'game_id' if it exists in the list\n",
    "categorical_bool_columns = list(categorical_bool_columns)\n",
    "if 'game_id' in categorical_bool_columns:\n",
    "    categorical_bool_columns.remove('game_id')\n",
    "\n",
    "categorical_bool_columns + ['home']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715394de-6f0a-479c-8acd-4d346774e0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c5a0cb-873b-41ab-8430-1feba4fecfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_data['Suggestion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f34c2a-a109-4346-89df-0a3e47ba5813",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_safeColumns =  list(vif_data[vif_data['Suggestion'].isin(['Keep: Moderately correlated',\n",
    "                                                              'Keep: Not correlated'])]['variables']) + categorical_bool_columns\n",
    "\n",
    "df1 = df[vif_safeColumns].reset_index(drop=True)\n",
    "\n",
    "def handle_duplicate_columns(df):\n",
    "    \"\"\" Remove duplicate columns by keeping the first occurrence \"\"\"\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    return df\n",
    "\n",
    "\n",
    "## remove duplicates columns\n",
    "df1 = handle_duplicate_columns(df1)\n",
    "\n",
    "df1['home'] = df['home']\n",
    "\n",
    "print(df1.info())\n",
    "\n",
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e8385e-adb1-4b65-99b9-ad5aa8d55e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data preparation function\n",
    "def prepare_data(data):\n",
    "    X = data.drop(columns=['won'])\n",
    "    y = data['won'].astype(int)\n",
    "\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', MinMaxScaler())])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)])\n",
    "\n",
    "    return X, y, preprocessor, numeric_cols, categorical_cols\n",
    "\n",
    "# Train and evaluate function\n",
    "def train_and_evaluate(X, y, preprocessor, numeric_cols, categorical_cols, top_features=20, model_type='logistic'):\n",
    "    if model_type == 'logistic':\n",
    "        classifier = LogisticRegression(solver='liblinear', random_state=42)\n",
    "    elif model_type == 'svm':\n",
    "        classifier = SVC(probability=True, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "\n",
    "    clf = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "\n",
    "    # Cross-validation with StratifiedKFold to prevent data leakage\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(clf, X, y, cv=skf, scoring='accuracy')\n",
    "    print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "    print(f\"Mean CV Accuracy: {np.mean(cv_scores)}\")\n",
    "\n",
    "    # Fit the model on the whole dataset\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Access the preprocessor step\n",
    "    preprocessor_fitted = clf.named_steps['preprocessor']\n",
    "    X_transformed = preprocessor_fitted.transform(X)\n",
    "\n",
    "    # Get feature names from the fitted preprocessor\n",
    "    feature_names = numeric_cols\n",
    "    \n",
    "    # Initialize SHAP explainer and compute SHAP values\n",
    "    explainer = shap.Explainer(clf.named_steps['classifier'], X_transformed)\n",
    "    shap_values = explainer.shap_values(X_transformed)\n",
    "\n",
    "    # Summarize the SHAP values to find the top features\n",
    "    shap_sum = np.abs(shap_values).mean(axis=0)\n",
    "    feature_importance = pd.DataFrame(list(zip(feature_names, shap_sum)), columns=['feature', 'shap_importance']).sort_values(by='shap_importance', ascending=False)\n",
    "\n",
    "    top_features_df = feature_importance.head(top_features).reset_index(drop=True)\n",
    "    print(\"Top Features Based on SHAP values:\\n\", top_features_df)\n",
    "\n",
    "    # Plotting SHAP values for top features\n",
    "    shap.summary_plot(shap_values, X_transformed, feature_names=feature_names)\n",
    "\n",
    "    return clf, explainer, shap_values, top_features_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c10477-a204-43a2-bc88-6a7bc95c0f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    '+/-_max', '+/-_max_opp',\n",
    "    'ortg_max', 'drtg_max',\n",
    "    'usg%_total', 'usg%_total_opp',\n",
    "    'home',\n",
    "    'fg%_max', 'fg%_max_opp',\n",
    "    '3p%_max', '3p%_max_opp',\n",
    "    'ft%_max', 'ft%_max_opp',\n",
    "    'fga_max', 'fga_max_opp',\n",
    "    '3pa_max', '3pa_max_opp',\n",
    "    'orb_max', 'orb_max_opp',\n",
    "    'drb%_max', 'drb%_max_opp',\n",
    "    'blk_max', 'blk_max_opp',\n",
    "    'tov_max', 'tov_max_opp',\n",
    "    'stl_max', 'stl_max_opp',\n",
    "    'pf_total', 'pf_total_opp',\n",
    "    'ast_max', 'ast_max_opp',\n",
    "    'ast%_max', 'ast%_max_opp',\n",
    "    '3par_max', '3par_max_opp',\n",
    "    'ftr_max', 'ftr_max_opp',\n",
    "    'blk%_max', 'blk%_max_opp',\n",
    "    'stl%_max', 'stl%_max_opp',\n",
    "]\n",
    "\n",
    "# Assuming df1 is your DataFrame\n",
    "df_selected = df[selected_features + ['won']]\n",
    "\n",
    "# Example usage\n",
    "# Assuming df_selected is your DataFrame with 'won' column included\n",
    "X, y, preprocessor, numeric_cols, categorical_cols = prepare_data(df_selected)\n",
    "model, explainer, shap_values, top_features_df = train_and_evaluate(X, y, preprocessor, numeric_cols, categorical_cols, top_features=20, model_type='logistic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd749db-8a86-45c3-822f-ebde8ee56852",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ddc6e6-2530-47f3-820e-167ef72cae10",
   "metadata": {},
   "source": [
    "Domain short listed\n",
    "\n",
    "1. usg%_max (Usage Rate Max): Measures the percentage of team plays a player uses while on the floor.\n",
    "2. trb%_max (Total Rebound Percentage Max): Indicates a player's or team's efficiency in grabbing available rebounds.\n",
    "3. ts%_max (True Shooting Percentage Max): An overall measure of shooting efficiency.\n",
    "4. pts_max (Points Max): Highest points scored in a game.\n",
    "5. ast_total (Total Assists): Total assists provided in a game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9739d36c-2f14-4171-8547-c245223bc125",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filtering only on SHAP Values\n",
    "\n",
    "df2 = df[list(top_features_df['feature']) + ['home',\"season\", \"date\", \"won\", \"team\",\n",
    "                                                      \"team_opp\"] + ['usg%_max','trb%_max','ts%_max', 'pts_max', 'ast_total']] ## Domain \n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee02234f-7a96-400a-94c1-a32d961e2ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(top_features_df['feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc17ce-e19d-4668-bcaa-8c418592e746",
   "metadata": {},
   "source": [
    "## Prepped Data Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f032833-14b9-45b5-bb3e-f1612de17e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for key metrics\n",
    "key_metrics = ['+/-_max', '+/-_max_opp', 'pf_total', 'pf_total_opp','fg%_max_opp','usg%_max','trb%_max','ts%_max', 'pts_max', 'ast_total']\n",
    "\n",
    "# Selecting the key metrics and generating descriptive statistics\n",
    "key_stats_summary = df2[key_metrics].describe()\n",
    "\n",
    "# Display the descriptive statistics for key metrics\n",
    "key_stats_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e65fd2f-1b85-446a-bd9e-aa83b9532409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for key metrics\n",
    "key_metrics_opp = ['+/-_max', '+/-_max_opp', 'pf_total', 'pf_total_opp','fg%_max_opp','usg%_max','trb%_max','ts%_max', 'pts_max', 'ast_total']\n",
    "\n",
    "# Selecting the key metrics and generating descriptive statistics\n",
    "key_stats_summary_opp = df2[key_metrics_opp].describe()\n",
    "\n",
    "# Display the descriptive statistics for key metrics\n",
    "key_stats_summary_opp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002f09ff-07e0-4514-8702-4cb595928984",
   "metadata": {},
   "source": [
    "1. Points Differential (+/-):\n",
    "The average points differential for teams (+/-_max: 13.45) and their opponents (+/-_max_opp: 13.09) suggests closely matched games.\n",
    "2. Usage and Rebounds (usg%_max, trb%_max):\n",
    "Teams have an average usage rate of 35.14% and a total rebound percentage of 25.11%, indicating the involvement of key players in scoring and rebounding opportunities.\n",
    "3. Shooting Efficiency (ts%_max):\n",
    "Teams achieve a true shooting percentage (accounting for field goals, 3-pointers, and free throws) of 96.38% on average, highlighting their overall scoring efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf490a3-fc9e-41f0-83b0-a6b105f87e75",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5ec433-149f-4e6e-be43-952117f87c73",
   "metadata": {},
   "source": [
    "Approach 1: \n",
    "1. What statistical indicators are most influential in determining the winning team?\n",
    "2. Can historical NBA game statistics be utilized to predict the outcome of a game? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fcf580-35ae-4128-a763-559d9fd1b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate_models(X, y, scaler,Approach, models):\n",
    "    \"\"\"Train and evaluate models using TimeSeriesSplit, handling multicollinearity, and return model metrics.\"\"\"\n",
    "    model_metrics = {}\n",
    "\n",
    "    ### Scaling the data\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_filtered = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    for name, model in models.items():\n",
    "        scores = []\n",
    "        detailed_reports = []\n",
    "        for train_index, test_index in tscv.split(X_filtered):\n",
    "            X_train, X_test = X_filtered.iloc[train_index], X_filtered.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            score = accuracy_score(y_test, y_pred)\n",
    "            scores.append(score)\n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "            detailed_reports.append(report)\n",
    "\n",
    "        avg_score = np.mean(scores)\n",
    "        model_metrics[name] = {\n",
    "            'Model': model,\n",
    "            'CV Scores': scores,\n",
    "            'Average CV Score': avg_score,\n",
    "            'Classification Reports': detailed_reports\n",
    "        }\n",
    "        print(f\"{name} Model Metrics:\")\n",
    "        print(\"Cross-Validation Scores:\", scores)\n",
    "        print(\"Average CV Score:\", avg_score)\n",
    "        print(\"Classification Reports for the last split:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        model_directory = 'models'\n",
    "        os.makedirs(model_directory, exist_ok=True)\n",
    "        model_path = f\"{model_directory}/{name}_Approach{Approach}.pkl\"\n",
    "        with open(model_path, 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "\n",
    "\n",
    "    return model_metrics, X_filtered, y\n",
    "\n",
    "\n",
    "# Example usage would be the same as before, initializing and calling these functions accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc8bf9-213b-419c-b394-8ea7345b231a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7440d0-1d18-4ebc-a789-cdd0116c121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, cv):\n",
    "    \"\"\"Plot and save the learning curve for the given estimator.\"\"\"\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 5))\n",
    "    \n",
    "    # Calculate means and standard deviations for the training and test sets\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0.4, 1.1)\n",
    "    plt.yticks(np.arange(0.4, 1.1, 0.1))\n",
    "    plt.grid()\n",
    "\n",
    "    # Plot the standard deviation as a shaded area around the mean\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    \n",
    "    # Plot the mean score for training and test sets\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig(f'{title}.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def format_classification_report(reports):\n",
    "    \"\"\"Formats and prints average classification report from multiple splits.\"\"\"\n",
    "    print(\"Average Classification Metrics:\")\n",
    "    for key in ['precision', 'recall', 'f1-score']:\n",
    "        avg_metric = np.mean([report['macro avg'][key] for report in reports])\n",
    "        print(f\"{key.capitalize()}: {avg_metric:.2f}\")\n",
    "\n",
    "def select_and_plot_best_model(model_metrics, X, y):\n",
    "    \"\"\"Select the best model based on CV scores and plot its learning curve.\"\"\"\n",
    "    best_model_name = max(model_metrics, key=lambda x: model_metrics[x]['Average CV Score'])\n",
    "    best_model = model_metrics[best_model_name]['Model']\n",
    "\n",
    "    print(f\"Selected Best Model: {best_model_name}\")\n",
    "    print(\"Cross-Validation Scores:\", model_metrics[best_model_name]['CV Scores'])\n",
    "    print(f\"Average CV Score: {model_metrics[best_model_name]['Average CV Score']}\")\n",
    "    format_classification_report(model_metrics[best_model_name]['Classification Reports'])\n",
    "\n",
    "    plot_learning_curve(best_model, f\"Learning Curve for {best_model_name}\", X, y, cv=StratifiedKFold(n_splits=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659f978-c96d-4b47-a2b6-a7d0a0fff45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3184fe-6e55-4820-b807-12e1cadc76a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the datatypes of the X values\n",
    "df_approach1 = df2.copy()\n",
    "df_approach1['date'] = pd.to_datetime(df_approach1['date'])\n",
    "df_approach1.sort_values('date', inplace=True)\n",
    "predictors = [col for col in df_approach1.columns if col not in [\"season\", \"date\", \"won\", \"team\", \"team_opp\"]]\n",
    "test = df_approach1[predictors]\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1c4ab-079f-40d3-bd0b-5cc4d5a58f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed26edbf-8857-42e8-bae8-49e15b2bdf78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36237362-89b1-45ee-a147-967f10681367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "df_approach1 = df2.copy()\n",
    "df_approach1['date'] = pd.to_datetime(df_approach1['date'])\n",
    "df_approach1.sort_values('date', inplace=True)\n",
    "predictors = [col for col in df_approach1.columns if col not in [\"season\", \"date\", \"won\", \"team\", \"team_opp\"]]\n",
    "X = df_approach1[predictors]\n",
    "y = df_approach1['won']\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b35455-691d-49d1-8525-9d6af7249d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "models = {\n",
    "    'logistic_regression': LogisticRegression(\n",
    "        penalty='l2',  # 'l1' or 'elasticnet' also possible\n",
    "        C=0.01,  # Regularization strength (smaller values specify stronger regularization)\n",
    "        max_iter=1000,\n",
    "        solver='liblinear'  # Suitable solver for 'l1' penalty\n",
    "    ),\n",
    "    'svm': SVC(\n",
    "        probability=True,\n",
    "        C=0.05,  # Regularization strength (smaller values specify stronger regularization)\n",
    "        kernel='rbf',  # Commonly used kernel with SVMs\n",
    "        gamma='scale'  # Kernel coefficient\n",
    "    ),\n",
    "'xgboost' : XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    alpha=0.05,  # Increased L1 regularization\n",
    "    reg_lambda=0.05,  # Increased L2 regularization\n",
    "    max_depth=4,\n",
    "    min_child_weight=5,\n",
    "    learning_rate=0.05\n",
    ")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbea274-08d9-4014-9331-85ecab90159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "start_dt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Started at = \", start_dt_string)\n",
    "\n",
    "###\n",
    "approach1_model_metrics, approach1_X_filtered, approach1_y = train_and_evaluate_models(X, y, scaler, Approach = 1, models = models)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "end_dt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Ended at = \", end_dt_string)\n",
    "print(f\"Total processing time: {(end_time - start_time) / 60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563d75c-f885-4f7d-93c2-c8f04e32c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "start_dt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Started at = \", start_dt_string)\n",
    "\n",
    "#### \n",
    "select_and_plot_best_model(approach1_model_metrics, approach1_X_filtered, approach1_y)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "end_dt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Ended at = \", end_dt_string)\n",
    "print(f\"Total processing time: {(end_time - start_time) / 60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab72b47-a62e-4728-adfd-d8aa0110575f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b66c41-3be3-4982-9eed-0eca0472372d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a0fb57f-6d24-4e8a-a0a7-b10a735aafd8",
   "metadata": {},
   "source": [
    "Approach 2:\n",
    "\n",
    "3. Can the prediction model adapt dynamically to in-season changes such as player \n",
    "performance trends? \n",
    "4. How does the home-court advantage factor into the predictive model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c65c2-3714-480d-99b8-14ef631d933e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf09a6-4915-4e1a-b473-6f3b13cfadd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assume approach2_df2 is your DataFrame already loaded and cleaned\n",
    "approach2_df2 = df2.copy()\n",
    "approach2_df2['date'] = pd.to_datetime(approach2_df2['date'])\n",
    "approach2_df2.sort_values(by=['team', 'date'], inplace=True)\n",
    "\n",
    "# Define predictors excluding non-numeric and irrelevant columns\n",
    "predictors = [col for col in approach2_df2.columns if col not in [\"season\", \"date\", \"won\", \"team\", \"team_opp\"]]\n",
    "\n",
    "# Calculate rolling averages for the past 3 games for all numeric predictors\n",
    "for predictor in predictors:\n",
    "    approach2_df2[f'{predictor}_rolling_avg'] = approach2_df2.groupby('team')[predictor].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
    "\n",
    "# Convert the 'home' column to an integer type if it's not already\n",
    "approach2_df2['home_game'] = approach2_df2['home'].astype(int)\n",
    "\n",
    "# Select the rolling average features and the home game feature\n",
    "feature_columns = [f'{predictor}_rolling_avg' for predictor in predictors] + ['home_game']\n",
    "approach2_X = approach2_df2[feature_columns]\n",
    "approach2_y = approach2_df2['won'].astype(int)  # Ensure target variable 'won' is integer\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc08839e-ff01-4a68-8bd1-8ceb434bca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a70dd-6aa0-4de1-a0bd-a42ec1aff479",
   "metadata": {},
   "outputs": [],
   "source": [
    "approach2_X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6564bd15-82d2-4085-9dce-fb7d86e8944d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d8cd7-564f-47e8-9030-280ea973e41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f14f5c-5cc5-483a-be7e-e513df7b986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time\n",
    "start_time = time.time()\n",
    "start_dt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Started at = \", start_dt_string)\n",
    "\n",
    "###\n",
    "approach2_model_metrics, approach2_X_filtered, approach2_y = train_and_evaluate_models(approach2_X, approach2_y, scaler, Approach = 2, models = models)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "end_dt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Ended at = \", end_dt_string)\n",
    "print(f\"Total processing time: {(end_time - start_time) / 60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ec5e52-acb9-4c4e-83d9-56105ad78e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a55dd82-ada0-4adf-a042-7f12cb26f2dc",
   "metadata": {},
   "source": [
    "Approach 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86490644-719d-491d-8b59-4ccf0ace2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "approach3_df2 = df2.copy()\n",
    "approach3_df2['date'] = pd.to_datetime(approach3_df2['date'])\n",
    "approach3_df2.sort_values(by=['team', 'date'], inplace=True)\n",
    "\n",
    "# Define predictors excluding non-numeric and irrelevant columns\n",
    "predictors = [col for col in approach3_df2.columns if col not in [\"season\", \"date\", \"won\", \"team\", \"team_opp\"]]\n",
    "\n",
    "# Add suffix '_original' to distinguish base features\n",
    "original_predictors = [f'{col}_original' for col in predictors]\n",
    "approach3_df2.rename(columns=dict(zip(predictors, original_predictors)), inplace=True)\n",
    "\n",
    "# Calculate rolling averages for the past 3 games for all numeric predictors\n",
    "for predictor in original_predictors:\n",
    "    approach3_df2[f'{predictor}_rolling_avg'] = approach3_df2.groupby('team')[predictor].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
    "\n",
    "# Convert the 'home' column to an integer type if it's not already\n",
    "approach3_df2['home_game'] = df2['home'].astype(int)\n",
    "\n",
    "# Select both original features and rolling average features, including 'home_game'\n",
    "combined_features = original_predictors + [f'{predictor}_rolling_avg' for predictor in original_predictors] + ['home_game']\n",
    "approach3_X = approach3_df2[combined_features]\n",
    "approach3_y = approach3_df2['won'].astype(int)  # Ensure target variable 'won' is integer\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049ee74-999a-49e0-8587-a00e7ef7d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "approach3_X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d2b24-21be-48b8-9a96-f9911af30dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b68f52-ba9e-43bf-ac4c-c57f6d142352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13262382-902e-48e0-9e27-e1fd3d49d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time\n",
    "start_time = time.time()\n",
    "start_dt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Started at = \", start_dt_string)\n",
    "\n",
    "###\n",
    "approach3_model_metrics, approach3_X_filtered, approach3_y = train_and_evaluate_models(approach3_X, approach3_y, scaler, Approach = 3, models = models)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "end_dt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Ended at = \", end_dt_string)\n",
    "print(f\"Total processing time: {(end_time - start_time) / 60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fba101-e1d0-415e-9fa2-f99a1e917132",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc036d9f-0554-486f-a374-16a7989f075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time\n",
    "start_time = time.time()\n",
    "start_dt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Started at = \", start_dt_string)\n",
    "\n",
    "#### \n",
    "select_and_plot_best_model(approach3_model_metrics, approach3_X_filtered, approach3_y)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "end_dt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Ended at = \", end_dt_string)\n",
    "print(f\"Total processing time: {(end_time - start_time) / 60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b1bd47-a85c-420b-aaa0-ce8f646f7aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d83c7-fe7d-4fe7-ad58-ed79fc8c31b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "817f2a37-03bc-40af-ad51-576e17be2149",
   "metadata": {},
   "source": [
    "## Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f955cd-b233-495c-8d4c-4c1ba8bf9827",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define ensemble models\n",
    "ensemble_models = {\n",
    "    'stacking': StackingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(\n",
    "                penalty='l2', C=0.01, max_iter=1000, solver='liblinear'\n",
    "            )),\n",
    "            ('svm', SVC(\n",
    "                probability=True, C=0.05, kernel='rbf', gamma='scale'\n",
    "            )),\n",
    "            ('xgb', XGBClassifier(\n",
    "                use_label_encoder=False, eval_metric='mlogloss',\n",
    "                alpha=0.05, reg_lambda=0.05, max_depth=4,\n",
    "                min_child_weight=5, learning_rate=0.05\n",
    "            ))\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(penalty='l2', C=0.01, max_iter=1000, solver='liblinear')\n",
    "    ),\n",
    "    'voting': VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(\n",
    "                penalty='l2', C=0.01, max_iter=1000, solver='liblinear'\n",
    "            )),\n",
    "            ('svm', SVC(\n",
    "                probability=True, C=0.05, kernel='rbf', gamma='scale'\n",
    "            )),\n",
    "            ('xgb', XGBClassifier(\n",
    "                use_label_encoder=False, eval_metric='mlogloss',\n",
    "                alpha=0.05, reg_lambda=0.05, max_depth=4,\n",
    "                min_child_weight=5, learning_rate=0.05\n",
    "            ))\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Utility function to load a model from a file\n",
    "def load_model(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "# Utility function to evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y, model_name):\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    print(f'{model_name} - Accuracy: {np.mean(scores):.2f} ± {np.std(scores):.2f}')\n",
    "    return np.mean(scores), scores\n",
    "\n",
    "# Function to plot and save the learning curve for the given estimator\n",
    "def plot_learning_curve(estimator, title, X, y, cv):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5))\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0.0, 1.1)\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig(f'{title}.png')\n",
    "    plt.show()\n",
    "\n",
    "# Function to evaluate and compare ensemble models (Stacking and Voting)\n",
    "def ensemble_approach(X, y, ensemble_models):\n",
    "    # Create directory for saving models\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "\n",
    "    for name, model in ensemble_models.items():\n",
    "        # Fit and evaluate each ensemble model\n",
    "        model.fit(X, y)\n",
    "        model_mean_score, model_scores = evaluate_model(model, X, y, f\"{name.capitalize()} Model\")\n",
    "\n",
    "        # Save the ensemble model\n",
    "        model_path = f\"models/{name.capitalize()}Model.pkl\"\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "        # Plot learning curve for the ensemble\n",
    "        plot_learning_curve(model, f\"Learning Curve for {name.capitalize()} Model\", X, y, StratifiedKFold(n_splits=5))\n",
    "\n",
    "        # Print classification report for the ensemble\n",
    "        y_pred = model.predict(X)\n",
    "        print(f\"Classification Report for {name.capitalize()} Model:\")\n",
    "        print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ffdd1-393a-4257-b21c-ee0ddae1d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "start_dt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Started at = \", start_dt_string)\n",
    "\n",
    "\n",
    "ensemble_approach(approach3_X_filtered, approach3_y, ensemble_models)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "end_dt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Ended at = \", end_dt_string)\n",
    "print(f\"Total processing time: {(end_time - start_time) / 60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcdbd14-a8cc-4aa0-84d2-5469d1a7d6f4",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "\n",
    "Among the two ensemble methods, Stacking Model and Voting Model, the Stacking Model has shown superior performance for this use case. It achieved a higher cross-validation score (0.77 ± 0.03) compared to the Voting Model (0.73 ± 0.03) and displayed a well-balanced learning curve, indicating a better generalization and less overfitting.\n",
    "\n",
    "Model Performance:\n",
    "XGBoost: Achieved an average cross-validation accuracy of 74%.\n",
    "Ensemble Model: Combining Logistic Regression, SVM, and XGBoost via stacking improved accuracy to 77% (±0.03).\n",
    "\n",
    "Feature Importance:\n",
    "SHAP Analysis: Revealed the most influential features include ± max, ± max_opp, pf_total, pf_total_opp, and 3p_total, emphasizing shooting efficiency and offensive production.\n",
    "\n",
    "In-Season Adaptability:\n",
    "Rolling Averages: Incorporating rolling averages for key metrics helped the model adapt to in-season changes and capture player/team performance dynamics.\n",
    "\n",
    "Home-Court Advantage:\n",
    "Impact on Winning: Home teams showed a higher probability of winning compared to away teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcbbe2f-e8e7-4212-b43c-e7ea68ac32b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb77a34-cc92-419b-a90a-3f1f58083be3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
