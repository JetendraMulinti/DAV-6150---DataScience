{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5565b60-0dc9-4cad-99ee-0f2b05ce5456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_auc_score\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26be90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of GitHub raw URLs\n",
    "urls = [\n",
    "    'https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2020.csv',\n",
    "    'https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2021.csv',\n",
    "    'https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2022.csv',\n",
    "    'https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2023.csv'\n",
    "]\n",
    "\n",
    "all_cleaned_dataframes = []\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Load the dataframe from pickle data obtained from URL\n",
    "        dataframe = pd.read_csv(url)\n",
    "        \n",
    "        # Filter out columns that start with 'Unnamed:'\n",
    "        dataframe = dataframe.loc[:, ~dataframe.columns.str.startswith('Unnamed:')]\n",
    "\n",
    "        # Drop all columns that are entirely NA\n",
    "        dataframe = dataframe.dropna(axis=1, how='all')\n",
    "\n",
    "        # Add the cleaned dataframe to the list\n",
    "        all_cleaned_dataframes.append(dataframe)\n",
    "        \n",
    "        print(f\"Processed data from {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "df = pd.concat(all_cleaned_dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "### delete some more columns\n",
    "columns_to_delete = ['OT', 'OT_opp', '2OT', '3OT', '2OT_opp', '3OT_opp',\n",
    "                     ## '4OT', '4OT_opp',\n",
    "                    'mp_total_opp','bpm_max','bpm_max_opp']\n",
    "\n",
    "# Drop the specified columns from the dataframe\n",
    "df.drop(columns=columns_to_delete, inplace=True)\n",
    "\n",
    "\n",
    "print(\"No of duplicate rows: \",df.duplicated().sum())\n",
    "\n",
    "### Drop duplicates\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(\"No of duplicate rows after dropping duplicates: \",df.duplicated().sum())\n",
    "\n",
    "#### rename columns\n",
    "df.rename(columns = {'mp_total':'mp'}, inplace=True)\n",
    "\n",
    "#### Creating Season column\n",
    "df['date'] = pd.to_datetime(df['date'])  # Convert 'date' column to datetime if it's not already\n",
    "\n",
    "# Function to determine the season year based on the month\n",
    "def get_season_year(row):\n",
    "    if row['date'].month >= 10:\n",
    "        return row['date'].year\n",
    "    else:\n",
    "        return row['date'].year - 1\n",
    "\n",
    "# Apply the function to create a new 'season' column\n",
    "df['season'] = df.apply(get_season_year, axis=1)\n",
    "\n",
    "\n",
    "print(\"data shape:\", df.shape)\n",
    "\n",
    "columns_format = list(df.columns)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994cb00a-72d5-4013-bd3f-98d9655490aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083779e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Abbrivate the Team names\n",
    "team_df = pd.read_csv('https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/Team_full-forms.csv')\n",
    "team_df['team'] = team_df['team'].str.strip()\n",
    "team_df['team1'] = team_df['team1'].str.strip()\n",
    "\n",
    "\n",
    "##### Merge and delete the columns\n",
    "df = pd.merge(team_df, df, on = ['team'], how='inner')\n",
    "del df['team']\n",
    "df.rename(columns = {'team1':'team'}, inplace=True)\n",
    "\n",
    "team_df.rename(columns = {'team':'team_opp'}, inplace=True)\n",
    "df = pd.merge(team_df, df, on = ['team_opp'], how='inner')\n",
    "del df['team_opp']\n",
    "df.rename(columns = {'team1':'team_opp'}, inplace=True)\n",
    "\n",
    "print(\"data shape:\", df.shape)\n",
    "\n",
    "df = df[columns_format]\n",
    "\n",
    "## ordering with date\n",
    "df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "df = df.sort_values(by = ['date'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4530f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68386cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### checking the data is balance / Imbalanced\n",
    "\n",
    "df['won'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded602c",
   "metadata": {},
   "source": [
    "Checking Null values and dropping columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07971fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checking null values\n",
    "\n",
    "null_columns = df.isnull().sum()\n",
    "null_columns[null_columns > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### delete some more columns\n",
    "more_columns_to_delete = ['index_opp']\n",
    "\n",
    "# Drop the specified columns from the dataframe\n",
    "df.drop(columns=more_columns_to_delete, inplace=True)\n",
    "\n",
    "## as we have only 1 null row (match) we will drop it\n",
    "df = df.dropna()\n",
    "\n",
    "null_columns = df.isnull().sum()\n",
    "null_columns[null_columns > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c70da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## re-ordering on date\n",
    "\n",
    "## ordering with date\n",
    "df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "df = df.sort_values(by = ['date'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "print(\"data shape:\", df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b7d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for key metrics\n",
    "key_metrics = ['fg_total', 'fga_total', 'fg%_total', '3p_total', '3pa_total', '3p%_total', 'ft_total',\n",
    "               'fta_total', 'ft%_total', 'total_opp']\n",
    "\n",
    "# Selecting the key metrics and generating descriptive statistics\n",
    "key_stats_summary = df[key_metrics].describe()\n",
    "\n",
    "# Display the descriptive statistics for key metrics\n",
    "key_stats_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800bcae8",
   "metadata": {},
   "source": [
    "1. Field Goals Made and Attempted (fg_total, fga_total): Teams make an average of 40 field goals per game from 87 attempts, translating to an average field goal percentage of 46.2%.\n",
    "2. Three-Point Shots (3p_total, 3pa_total, 3p%_total): On average, teams successfully make 11 three-point shots per game from 31 attempts, achieving a three-point shooting percentage of 35.7%.\n",
    "3. Free Throws (ft_total, fta_total, ft%_total): Teams typically make 17 free throws per game from 23 attempts, with an average success rate of 77.2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f6625",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "\n",
    "# Plotting field goals, three-point shots, and free throws\n",
    "sns.histplot(df['fg_total'], bins=30, kde=True, ax=axes[0, 0]).set_title('Field Goals Made')\n",
    "sns.histplot(df['3p_total'], bins=30, kde=True, ax=axes[0, 1]).set_title('Three-Points Made')\n",
    "sns.histplot(df['ft_total'], bins=30, kde=True, ax=axes[0, 2]).set_title('Free Throws Made')\n",
    "\n",
    "# Plotting percentages for field goals, three-point shots, and free throws\n",
    "sns.histplot(df['fg%_total'], bins=30, kde=True, ax=axes[1, 0]).set_title('Field Goal Percentage')\n",
    "sns.histplot(df['3p%_total'], bins=30, kde=True, ax=axes[1, 1]).set_title('Three-Point Percentage')\n",
    "sns.histplot(df['ft%_total'], bins=30, kde=True, ax=axes[1, 2]).set_title('Free Throw Percentage')\n",
    "\n",
    "# Plotting games per season and distributions of win and next game outcomes\n",
    "sns.histplot(df['season'], bins=len(df['season'].unique()), kde=False, ax=axes[2, 0]).set_title('Games per Season')\n",
    "sns.countplot(x='won', data=df, ax=axes[2, 1]).set_title('Win Distribution')\n",
    "## sns.countplot(x='target', data=df, ax=axes[2, 2]).set_title('Next Game Outcome Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2badae-c404-4518-aef1-73e200e4a40f",
   "metadata": {},
   "source": [
    "1. Field Goals Made and Three-Points Made distributions center around a common range, indicating a pattern in scoring strategies across games.\n",
    "2. Percentage metrics for Field Goals, Three-Points, and Free Throws exhibit a normal distribution, reflecting a standard level of efficiency across matches.\n",
    "3. The Games per Season distribution shows consistency in the number of games played, which supports analyses over multiple seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783381d1-d413-45c4-b1db-3657936504da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_trend(column):\n",
    "    # Check if the column data looks like percentages (values between 0 and 1)\n",
    "    if df[column].max() <= 1:\n",
    "        # If so, convert to percentage by multiplying by 100\n",
    "        seasonal_averages = df.groupby('season')[column].mean() * 100\n",
    "        ylabel = f'Average {column} (%)'\n",
    "    else:\n",
    "        # Otherwise, use the values as is\n",
    "        seasonal_averages = df.groupby('season')[column].mean()\n",
    "        ylabel = f'Average {column}'\n",
    "    \n",
    "    # Plotting the time series\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    seasonal_averages.plot(kind='line', marker='o')\n",
    "    plt.title(f'Average {column} by NBA Season')\n",
    "    plt.xlabel('NBA Season')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    plt.xticks(ticks=seasonal_averages.index, labels=seasonal_averages.index)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed311a65-ea3e-4fa2-bd05-bb2dda415118",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_trend('fg%_total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf10d37-ee30-4c57-84e3-2127779e15f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50949a05-894b-4bf7-9073-24409a530318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_team_performance( metric):\n",
    "    # Determine if the metric is a percentage (between 0 and 1)\n",
    "    percentage_scale = df[metric].max() <= 1\n",
    "    \n",
    "    # Calculate the average metric for each team per season\n",
    "    seasonal_team_averages = df.groupby(['season', 'team'])[metric].mean().unstack()\n",
    "\n",
    "    # Scale up if the metric is a percentage\n",
    "    if percentage_scale:\n",
    "        seasonal_team_averages *= 100\n",
    "        ylabel = f'Average {metric} (%)'\n",
    "    else:\n",
    "        ylabel = f'Average {metric}'\n",
    "\n",
    "    # Identify the top and bottom 5 performing teams\n",
    "    top_teams = seasonal_team_averages.mean(axis=0).sort_values(ascending=False).head(5).index\n",
    "    bottom_teams = seasonal_team_averages.mean(axis=0).sort_values(ascending=True).head(5).index\n",
    "\n",
    "    # Create subplots for the top and bottom performing teams\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "    # Top 5 performing teams plot\n",
    "    for team in top_teams:\n",
    "        axs[0].plot(seasonal_team_averages.index, seasonal_team_averages.loc[:, team], marker='o', label=team)\n",
    "    axs[0].set_title(f'Top 5 Performing Teams by {metric}')\n",
    "    axs[0].set_ylabel(ylabel)\n",
    "    axs[0].grid(True)\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Bottom 5 performing teams plot\n",
    "    for team in bottom_teams:\n",
    "        axs[1].plot(seasonal_team_averages.index, seasonal_team_averages.loc[:, team], marker='o', label=team)\n",
    "    axs[1].set_title(f'Bottom 5 Performing Teams by {metric}')\n",
    "    axs[1].set_ylabel(ylabel)\n",
    "    axs[1].grid(True)\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Set common X label\n",
    "    plt.xlabel('NBA Season')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671cc75d-fcf6-4e32-a852-1445c9e5dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_team_performance('fg%_total') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da8edf-714a-4e6b-8b04-f819f3d9742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ec512-6d49-49a6-b992-7d9f4d21b312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144e67f5-0bba-4a68-a5cf-5d37fd5bdda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# List of column names correctly passed to the correlation matrix plotting\n",
    "columns_to_include = [\n",
    " 'mp', 'fg_total', 'fga_total', 'fg%_total', '3p_total', '3pa_total', '3p%_total',\n",
    " 'ft_total', 'fta_total', 'ft%_total', 'orb_total', 'drb_total', 'trb_total',\n",
    " 'ast_total', 'stl_total', 'blk_total', 'tov_total', 'pf_total', 'pts_total',\n",
    " 'ts%_total', 'efg%_total', '3par_total', 'ftr_total', 'orb%_total', 'drb%_total',\n",
    " 'trb%_total', 'ast%_total', 'stl%_total', 'blk%_total', 'tov%_total',\n",
    " 'usg%_total', 'ortg_total', 'drtg_total', 'home', 'won'  # Including only relevant columns\n",
    "]\n",
    "\n",
    "# Ensure that all these columns exist in df before using them\n",
    "if set(columns_to_include).issubset(df.columns):\n",
    "    fig, ax = plt.subplots(figsize=(25, 25))\n",
    "    correlation_matrix = df[columns_to_include].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', ax=ax)\n",
    "    ax.set_title('Correlation Matrix of Selected Metrics with Target')\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plt.savefig('CorrelationMatrix.png')\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Some columns are missing in the DataFrame. Please check the column names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b92b45-bb2e-4377-85a3-0cdb74de1d1f",
   "metadata": {},
   "source": [
    "Multicollinearity Check Using Variance Inflation Factor (VIF)\n",
    "Purpose: Assessing multicollinearity among predictive features to ensure that the model is not unduly influenced by highly correlated independent variables. This helps in refining the model to improve prediction accuracy and reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfaa19e-6938-42a3-a139-17d19717bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Function to calculate VIF for each feature and provide specific suggestions\n",
    "def calculate_vif(data):\n",
    "    vif_df = pd.DataFrame()\n",
    "    vif_df[\"variables\"] = data.columns\n",
    "    vif_df[\"VIF\"] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n",
    "    \n",
    "    # Defining suggestions based on VIF values\n",
    "    def vif_suggestions(vif_value):\n",
    "        if vif_value <= 1:\n",
    "            return \"Keep: Not correlated\"\n",
    "        elif 1 < vif_value < 5:\n",
    "            return \"Keep: Moderately correlated\"\n",
    "        elif 5 <= vif_value < 10:\n",
    "            return \"Consider reviewing: Highly correlated\"\n",
    "        else:\n",
    "            return \"Remove or transform: Very high correlation\"\n",
    "\n",
    "    vif_df['Suggestion'] = vif_df['VIF'].apply(vif_suggestions)\n",
    "    return vif_df\n",
    "\n",
    "# Selecting numeric features for VIF calculation\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "vif_data = calculate_vif(df[numeric_cols].dropna())\n",
    "\n",
    "# Display VIF scores\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False).reset_index(drop=True)\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f02e98f-0152-45e9-bff7-ebdc8d0cfc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_data['Suggestion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b8913e-c3c2-49de-9567-51a3cba153fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f20375-bd3d-422f-b7f8-36624aa7ab1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ed04525-45e6-4d05-8200-f812d5a28693",
   "metadata": {},
   "source": [
    "Lagged Features for Dynamic Team Performance\n",
    "Purpose: Creating lagged features to assess how past game performances (e.g., points scored, rebounds) influence the outcome of future games. This analysis helps in understanding team momentum or fatigue, which can be crucial for predicting outcomes of future games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af036e4-1fda-4d8e-ad11-d1377b70edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the rolling windows you want to test\n",
    "rolling_windows = [1, 3, 5, 7, 10]\n",
    "\n",
    "# Assuming 'df' and 'pts_total' are already defined in your DataFrame\n",
    "# Create lagged features for each window size\n",
    "for window in rolling_windows:\n",
    "    # Create the lagged data for points\n",
    "    df[f'pts_scored_lag{window}'] = df.groupby('team')['pts_total'].shift(1).rolling(window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "\n",
    "# Drop rows with NaN values in the target column, which will appear at the end of each team's data\n",
    "df.dropna(subset=['won'], inplace=True)\n",
    "\n",
    "# Prepare the figure layout\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(14, 18))\n",
    "axes = axes.flatten()  # Flatten the array of axes for easier iteration\n",
    "\n",
    "# Plot the impact of each lagged feature on the target\n",
    "for i, window in enumerate(rolling_windows):\n",
    "    sns.boxplot(x='won', y=f'pts_scored_lag{window}', data=df, ax=axes[i])\n",
    "    axes[i].set_title(f'Impact of Average Points Last {window} Games Current Game Winning')\n",
    "    axes[i].set_xlabel('Game Won')\n",
    "    axes[i].set_ylabel(f'Points Scored in Last {window} Games')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig('Impact_of_Lagged_Features_on_Winning.png')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc21969-1674-461b-83c7-e703a7c82e8a",
   "metadata": {},
   "source": [
    "From the visual inspection, Lag 3 & Lag 1(average points from the last 3 games & last game) seems to provide the best balance between capturing enough historical performance to predict future outcomes and not including too much past data which dilutes the predictive power. The slight increase in median points for games that were won suggests that averaging over three games strikes a good balance in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eefcd62-24be-4046-b6f9-1a8e9ccf73fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff7aaa-4f61-4c14-a58d-891f43340ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plotting the impact of home-court advantage\n",
    "plt.figure(figsize=(6, 4))\n",
    "ax = sns.barplot(x='home', y='won', data=df, ci=None)  # ci=None to remove the confidence interval bars\n",
    "plt.title(\"Impact of Home Court on Winning\")\n",
    "plt.xlabel(\"Home Game (1 = Home, 0 = Away)\")\n",
    "plt.ylabel(\"Probability of Winning\")\n",
    "\n",
    "# Calculate the mean probabilities for annotations\n",
    "home_winning_probabilities = df.groupby('home')['won'].mean()\n",
    "\n",
    "# Annotate the bars with the calculated probabilities\n",
    "for i, p in enumerate(ax.patches):  # access the bars\n",
    "    ax.annotate(format(home_winning_probabilities.iloc[i], '.2f'),  # format the probability\n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()),  # position for the text\n",
    "                ha = 'center', va = 'center',  # center alignment\n",
    "                xytext = (0, 9),  # position text slightly above the bar\n",
    "                textcoords = 'offset points')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d505e575-833c-4ef4-879a-1e1d729739a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b1d3d-fe54-4c50-a990-106397f2a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns = df.isnull().sum()\n",
    "null_columns[null_columns > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97207551-3a23-4270-be9b-47fb8dd2ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "### delete null columns\n",
    "\n",
    "try:\n",
    "    df.drop(columns=['pts_scored_lag1','pts_scored_lag3','pts_scored_lag5',\n",
    "                    'pts_scored_lag7','pts_scored_lag10'], inplace=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec235a0-c0e5-49c0-9da9-f40cb8260379",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns = df.isnull().sum()\n",
    "null_columns[null_columns > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5ec433-149f-4e6e-be43-952117f87c73",
   "metadata": {},
   "source": [
    "Approach 1: \n",
    "1. What statistical indicators are most influential in determining the winning team?\n",
    "2. Can historical NBA game statistics be utilized to predict the outcome of a game? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa5d4ce-348a-419f-922e-624eff5c10c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d5bdb-4e2c-488e-b154-545747c2f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import clone\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "def train_and_evaluate_models_approach_1(df, target_column, scaler):\n",
    "    model_metrics = {}\n",
    "    # Prepare data\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column].astype(int)\n",
    "    # Preprocess features\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', scaler, numeric_cols),\n",
    "        ('cat', OneHotEncoder(), categorical_cols)\n",
    "    ])\n",
    "\n",
    "    models = {\n",
    "        'logistic_regression': LogisticRegression(max_iter=1000),\n",
    "       ## 'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'svm': SVC(probability=True),\n",
    "        'xgboost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    for name, model in models.items():\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        scores = []\n",
    "        reports = []\n",
    "\n",
    "        for train_index, test_index in tscv.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            score = accuracy_score(y_test, y_pred)\n",
    "            scores.append(score)\n",
    "            reports.append(classification_report(y_test, y_pred, output_dict=True))\n",
    "\n",
    "        avg_score = np.mean(scores)\n",
    "        model_metrics[name] = {\n",
    "            'Model': clone(model),\n",
    "            'CV Scores': scores,\n",
    "            'Average CV Score': avg_score,\n",
    "            'Classification Reports': reports\n",
    "        }\n",
    "        print(f\"{name} Model Metrics:\")\n",
    "        print(\"Cross-Validation Scores:\", scores)\n",
    "        print(\"Average CV Score:\", avg_score)\n",
    "        print(\"Classification Reports for Last Split:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        # Save the model\n",
    "        model_directory = 'models'\n",
    "        os.makedirs(model_directory, exist_ok=True)\n",
    "        model_path = os.path.join(model_directory, f'{name}_Approach1.pkl')\n",
    "        with open(model_path, 'wb') as file:\n",
    "            pickle.dump(pipeline, file)\n",
    "        print(f\"Model {name} saved at {model_path}\")\n",
    "\n",
    "    return model_metrics, X, y  # Also return X and y for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21534c1-a387-42cf-9738-286857c05e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_and_plot_best_model(model_metrics, X, y):\n",
    "    best_model_name = max(model_metrics, key=lambda x: model_metrics[x]['Average CV Score'])\n",
    "    best_model = model_metrics[best_model_name]['Model']\n",
    "\n",
    "    print(f\"Selected Best Model: {best_model_name}\")\n",
    "    print(\"Cross-Validation Scores:\", model_metrics[best_model_name]['CV Scores'])\n",
    "    print(f\"Average CV Score: {model_metrics[best_model_name]['Average CV Score']}\")\n",
    "    print(\"Classification Reports for Last Split:\")\n",
    "    last_report = model_metrics[best_model_name]['Classification Reports'][-1]\n",
    "    print(classification_report(y_true=list(last_report.keys()), y_pred=list(last_report.values())))\n",
    "\n",
    "    # Plot and save the learning curve\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        best_model, X, y, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 5)\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(f\"Learning Curve for {best_model_name}\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0.0, 1.1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "    # Save the best model separately\n",
    "    model_path = f'models/{best_model_name}_best_model.pkl'\n",
    "    with open(model_path, 'wb') as file:\n",
    "        pickle.dump(best_model, file)\n",
    "    print(f\"Best model ({best_model_name}) saved at {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf72138-b478-4968-b593-58cf11ca34c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "df_approach1 = df.copy()\n",
    "df_approach1['date'] = pd.to_datetime(df_approach1['date'])\n",
    "df_approach1.sort_values('date', inplace=True)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d287f4-9849-440b-819e-57fb68b55011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "start_dt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Started at = \", start_dt_string)\n",
    "\n",
    "### call the function train_and_evaluate_models_approach_1\n",
    "model_metrics, X, y = train_and_evaluate_models_approach_1(df_approach1, 'won', scaler)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "end_dt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Ended at = \", end_dt_string)\n",
    "print(f\"Total processing time: {(end_time - start_time) / 60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce64784-0705-4169-8f8c-656cb0c8cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "start_dt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Started at = \", start_dt_string)\n",
    "\n",
    "### call the function select_and_plot_best_model\n",
    "select_and_plot_best_model(model_metrics, X, y)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "end_dt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"Ended at = \", end_dt_string)\n",
    "print(f\"Total processing time: {(end_time - start_time) / 60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83347190-d194-4e90-82d3-fafd930d9adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1043a199-0158-4d0b-9c6f-dd780a7e3283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f14f5c-5cc5-483a-be7e-e513df7b986c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
