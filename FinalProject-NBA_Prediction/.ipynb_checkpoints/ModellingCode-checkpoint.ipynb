{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3563ad-17b3-4863-ac55-7647d8c71dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01327eab-cd98-42b6-b68d-b017dcae07a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved dataframes_list_season_-2016.pkl as season_-2016.csv\n",
      "Processed and saved dataframes_list_season_-2017.pkl as season_-2017.csv\n",
      "Processed and saved dataframes_list_season_-2018.pkl as season_-2018.csv\n",
      "Processed and saved dataframes_list_season_-2019.pkl as season_-2019.csv\n",
      "Processed and saved dataframes_list_season_-2020.pkl as season_-2020.csv\n",
      "Processed and saved dataframes_list_season_-2021.pkl as season_-2021.csv\n",
      "Processed and saved dataframes_list_season_-2022.pkl as season_-2022.csv\n",
      "Processed and saved dataframes_list_season_-2023.pkl as season_-2023.csv\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import pickle\n",
    "# import os\n",
    "\n",
    "# file_names = [\n",
    "#             'dataframes_list_season_-2016.pkl','dataframes_list_season_-2017.pkl',\n",
    "#              'dataframes_list_season_-2018.pkl', 'dataframes_list_season_-2019.pkl',\n",
    "#               'dataframes_list_season_-2020.pkl', 'dataframes_list_season_-2021.pkl',\n",
    "#               'dataframes_list_season_-2022.pkl','dataframes_list_season_-2023.pkl']\n",
    "\n",
    "\n",
    "# # Directory where the files are located\n",
    "# directory = \"Data/\"\n",
    "\n",
    "# for file_name in file_names:\n",
    "#     try:\n",
    "#         # Construct the full path to the file\n",
    "#         file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "#         # Load the dataframe from the pickle file\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             dataframe = pickle.load(file)\n",
    "\n",
    "#         # Extract the year from the file name\n",
    "#         # The year is located between the last dash and the dot before 'pkl'\n",
    "#         year = file_name.split('_')[-1].split('.')[0]  # Splits the file name and takes the year part\n",
    "\n",
    "#         # Define the CSV file name based on the year extracted\n",
    "#         csv_file_name = f\"season_{year}.csv\"\n",
    "\n",
    "#         # Save the dataframe to CSV in the specified directory\n",
    "#         dataframe.to_csv(os.path.join(directory, csv_file_name))\n",
    "\n",
    "#         print(f\"Processed and saved {file_name} as {csv_file_name}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {file_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26be90aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing https://github.com/JetendraMulinti/DAV-6150---DataScience/blob/main/FinalProject-NBA_Prediction/Data/dataframes_list_season_-2020.pkl: invalid load key, '\\x0a'.\n",
      "Error processing https://github.com/JetendraMulinti/DAV-6150---DataScience/blob/main/FinalProject-NBA_Prediction/Data/dataframes_list_season_-2021.pkl: invalid load key, '\\x0a'.\n",
      "Error processing https://github.com/JetendraMulinti/DAV-6150---DataScience/blob/main/FinalProject-NBA_Prediction/Data/dataframes_list_season_-2022.pkl: invalid load key, '\\x0a'.\n",
      "Error processing https://github.com/JetendraMulinti/DAV-6150---DataScience/blob/main/FinalProject-NBA_Prediction/Data/dataframes_list_season_-2023.pkl: invalid load key, '\\x0a'.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Concatenate all dataframes into one\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_cleaned_dataframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m### delete some more columns\u001b[39;00m\n\u001b[0;32m     71\u001b[0m columns_to_delete \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOT_opp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2OT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3OT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2OT_opp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3OT_opp\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     72\u001b[0m                      \u001b[38;5;66;03m## '4OT', '4OT_opp',\u001b[39;00m\n\u001b[0;32m     73\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmp_total_opp\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbpm_max\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbpm_max_opp\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\work\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:372\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    370\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 372\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\work\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:429\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    426\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    432\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# ### Reading and drop na columns\n",
    "# file_names = [\n",
    "#             ## 'dataframes_list_season_-2016.pkl','dataframes_list_season_-2017.pkl',\n",
    "#             ##  'dataframes_list_season_-2018.pkl', 'dataframes_list_season_-2019.pkl',\n",
    "#               'dataframes_list_season_-2020.pkl', 'dataframes_list_season_-2021.pkl',\n",
    "#               'dataframes_list_season_-2022.pkl','dataframes_list_season_-2023.pkl']\n",
    "\n",
    "# all_cleaned_dataframes = []\n",
    "\n",
    "# for file_name in file_names:\n",
    "#     try:\n",
    "#         with open(\"Data/\"+file_name, 'rb') as file:\n",
    "#             # Load the dataframe from pickle file\n",
    "#             dataframe = pickle.load(file)\n",
    "\n",
    "#             # Filter out columns that start with 'Unnamed:'\n",
    "#             dataframe = dataframe.loc[:, ~dataframe.columns.str.startswith('Unnamed:')]\n",
    "\n",
    "#             # Drop all columns that are entirely NA\n",
    "#             dataframe = dataframe.dropna(axis=1, how='all')\n",
    "\n",
    "#             # Add the cleaned dataframe to the list\n",
    "#             all_cleaned_dataframes.append(dataframe)\n",
    "            \n",
    "#             ## print(f\"Processed {file_name}, columns: {dataframe.columns}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "# # Concatenate all dataframes into one\n",
    "# df = pd.concat(all_cleaned_dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "# List of GitHub raw URLs\n",
    "urls = [\n",
    "    'https://github.com/JetendraMulinti/DAV-6150---DataScience/blob/main/FinalProject-NBA_Prediction/Data/dataframes_list_season_-2020.pkl',\n",
    "    'https://github.com/JetendraMulinti/DAV-6150---DataScience/blob/main/FinalProject-NBA_Prediction/Data/dataframes_list_season_-2021.pkl',\n",
    "    'https://github.com/JetendraMulinti/DAV-6150---DataScience/blob/main/FinalProject-NBA_Prediction/Data/dataframes_list_season_-2022.pkl',\n",
    "    'https://github.com/JetendraMulinti/DAV-6150---DataScience/blob/main/FinalProject-NBA_Prediction/Data/dataframes_list_season_-2023.pkl'\n",
    "]\n",
    "\n",
    "all_cleaned_dataframes = []\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises stored HTTPError, if one occurred\n",
    "        \n",
    "        # Load the dataframe from pickle data obtained from URL\n",
    "        dataframe = pd.read_pickle(BytesIO(response.content))\n",
    "        \n",
    "        # Filter out columns that start with 'Unnamed:'\n",
    "        dataframe = dataframe.loc[:, ~dataframe.columns.str.startswith('Unnamed:')]\n",
    "\n",
    "        # Drop all columns that are entirely NA\n",
    "        dataframe = dataframe.dropna(axis=1, how='all')\n",
    "\n",
    "        # Add the cleaned dataframe to the list\n",
    "        all_cleaned_dataframes.append(dataframe)\n",
    "        \n",
    "        print(f\"Processed data from {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "df = pd.concat(all_cleaned_dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "### delete some more columns\n",
    "columns_to_delete = ['OT', 'OT_opp', '2OT', '3OT', '2OT_opp', '3OT_opp',\n",
    "                     ## '4OT', '4OT_opp',\n",
    "                    'mp_total_opp','bpm_max','bpm_max_opp']\n",
    "\n",
    "# Drop the specified columns from the dataframe\n",
    "df.drop(columns=columns_to_delete, inplace=True)\n",
    "\n",
    "\n",
    "print(\"No of duplicate rows: \",df.duplicated().sum())\n",
    "\n",
    "### Drop duplicates\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(\"No of duplicate rows after dropping duplicates: \",df.duplicated().sum())\n",
    "\n",
    "#### rename columns\n",
    "df.rename(columns = {'mp_total':'mp'}, inplace=True)\n",
    "\n",
    "#### Creating Season column\n",
    "df['date'] = pd.to_datetime(df['date'])  # Convert 'date' column to datetime if it's not already\n",
    "\n",
    "# Function to determine the season year based on the month\n",
    "def get_season_year(row):\n",
    "    if row['date'].month >= 10:\n",
    "        return row['date'].year\n",
    "    else:\n",
    "        return row['date'].year - 1\n",
    "\n",
    "# Apply the function to create a new 'season' column\n",
    "df['season'] = df.apply(get_season_year, axis=1)\n",
    "\n",
    "\n",
    "print(\"data shape:\", df.shape)\n",
    "\n",
    "columns_format = list(df.columns)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "994cb00a-72d5-4013-bd3f-98d9655490aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://github.com/JetendraMulinti/DAV-6150---DataScience/blob/main/FinalProject-NBA_Prediction/Data/dataframes_list_season_-2020.pkl'\n",
    "\n",
    "requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083779e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Abbrivate the Team names\n",
    "team_df = pd.read_excel(\"Data/\"+'Team_full-forms.xlsx')\n",
    "team_df['team'] = team_df['team'].str.strip()\n",
    "team_df['team1'] = team_df['team1'].str.strip()\n",
    "\n",
    "\n",
    "##### Merge and delete the columns\n",
    "df = pd.merge(team_df, df, on = ['team'], how='inner')\n",
    "del df['team']\n",
    "df.rename(columns = {'team1':'team'}, inplace=True)\n",
    "\n",
    "team_df.rename(columns = {'team':'team_opp'}, inplace=True)\n",
    "df = pd.merge(team_df, df, on = ['team_opp'], how='inner')\n",
    "del df['team_opp']\n",
    "df.rename(columns = {'team1':'team_opp'}, inplace=True)\n",
    "\n",
    "print(\"data shape:\", df.shape)\n",
    "\n",
    "df = df[columns_format]\n",
    "\n",
    "## ordering with date\n",
    "df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "df = df.sort_values(by = ['date'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4530f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['season'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc69ab03",
   "metadata": {},
   "source": [
    "1. We will create a target column (needs to be created on team level) -> represents the next game outcome. (Won column indicates the current match, target column indicates next match)\n",
    "\n",
    "2. Replace the Null values in Target column with “2”, False (Loss) = 0, True (Won) = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf0b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target(team):\n",
    "    team['target'] = team['won'].shift(-1)\n",
    "    return team\n",
    "\n",
    "df = df.groupby(\"team\", group_keys=False).apply(add_target)\n",
    "\n",
    "## Preprocessing Target column (Null = 2, True = 1, False = 0)\n",
    "\n",
    "df['target'][pd.isnull(df['target'])] = 2\n",
    "df['target'] = df['target'].astype(int, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68386cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### checking the data is balance / Imbalanced\n",
    "\n",
    "df['won'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded602c",
   "metadata": {},
   "source": [
    "Checking Null values and dropping columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07971fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checking null values\n",
    "\n",
    "null_columns = df.isnull().sum()\n",
    "null_columns[null_columns > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### delete some more columns\n",
    "more_columns_to_delete = ['index_opp']\n",
    "\n",
    "# Drop the specified columns from the dataframe\n",
    "df.drop(columns=more_columns_to_delete, inplace=True)\n",
    "\n",
    "## as we have only 1 null row (match) we will drop it\n",
    "df = df.dropna()\n",
    "\n",
    "null_columns = df.isnull().sum()\n",
    "null_columns[null_columns > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c70da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## re-ordering on date\n",
    "\n",
    "## ordering with date\n",
    "df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "df = df.sort_values(by = ['date'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "print(\"data shape:\", df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b7d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for key metrics\n",
    "key_metrics = ['fg_total', 'fga_total', 'fg%_total', '3p_total', '3pa_total', '3p%_total', 'ft_total',\n",
    "               'fta_total', 'ft%_total', 'total_opp']\n",
    "\n",
    "# Selecting the key metrics and generating descriptive statistics\n",
    "key_stats_summary = df[key_metrics].describe()\n",
    "\n",
    "# Display the descriptive statistics for key metrics\n",
    "key_stats_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800bcae8",
   "metadata": {},
   "source": [
    "1. Field Goals Made and Attempted (fg_total, fga_total): Teams make an average of 40 field goals per game from 87 attempts, translating to an average field goal percentage of 46.2%.\n",
    "2. Three-Point Shots (3p_total, 3pa_total, 3p%_total): On average, teams successfully make 11 three-point shots per game from 31 attempts, achieving a three-point shooting percentage of 35.7%.\n",
    "3. Free Throws (ft_total, fta_total, ft%_total): Teams typically make 17 free throws per game from 23 attempts, with an average success rate of 77.2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f6625",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "\n",
    "# Plotting field goals, three-point shots, and free throws\n",
    "sns.histplot(df['fg_total'], bins=30, kde=True, ax=axes[0, 0]).set_title('Field Goals Made')\n",
    "sns.histplot(df['3p_total'], bins=30, kde=True, ax=axes[0, 1]).set_title('Three-Points Made')\n",
    "sns.histplot(df['ft_total'], bins=30, kde=True, ax=axes[0, 2]).set_title('Free Throws Made')\n",
    "\n",
    "# Plotting percentages for field goals, three-point shots, and free throws\n",
    "sns.histplot(df['fg%_total'], bins=30, kde=True, ax=axes[1, 0]).set_title('Field Goal Percentage')\n",
    "sns.histplot(df['3p%_total'], bins=30, kde=True, ax=axes[1, 1]).set_title('Three-Point Percentage')\n",
    "sns.histplot(df['ft%_total'], bins=30, kde=True, ax=axes[1, 2]).set_title('Free Throw Percentage')\n",
    "\n",
    "# Plotting games per season and distributions of win and next game outcomes\n",
    "sns.histplot(df['season'], bins=len(df['season'].unique()), kde=False, ax=axes[2, 0]).set_title('Games per Season')\n",
    "sns.countplot(x='won', data=df, ax=axes[2, 1]).set_title('Win Distribution')\n",
    "sns.countplot(x='target', data=df, ax=axes[2, 2]).set_title('Next Game Outcome Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2badae-c404-4518-aef1-73e200e4a40f",
   "metadata": {},
   "source": [
    "1. Field Goals Made and Three-Points Made distributions center around a common range, indicating a pattern in scoring strategies across games.\n",
    "2. Percentage metrics for Field Goals, Three-Points, and Free Throws exhibit a normal distribution, reflecting a standard level of efficiency across matches.\n",
    "3. The Games per Season distribution shows consistency in the number of games played, which supports analyses over multiple seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783381d1-d413-45c4-b1db-3657936504da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_trend(column):\n",
    "    # Check if the column data looks like percentages (values between 0 and 1)\n",
    "    if df[column].max() <= 1:\n",
    "        # If so, convert to percentage by multiplying by 100\n",
    "        seasonal_averages = df.groupby('season')[column].mean() * 100\n",
    "        ylabel = f'Average {column} (%)'\n",
    "    else:\n",
    "        # Otherwise, use the values as is\n",
    "        seasonal_averages = df.groupby('season')[column].mean()\n",
    "        ylabel = f'Average {column}'\n",
    "    \n",
    "    # Plotting the time series\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    seasonal_averages.plot(kind='line', marker='o')\n",
    "    plt.title(f'Average {column} by NBA Season')\n",
    "    plt.xlabel('NBA Season')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    plt.xticks(ticks=seasonal_averages.index, labels=seasonal_averages.index)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed311a65-ea3e-4fa2-bd05-bb2dda415118",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_trend('fg%_total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf10d37-ee30-4c57-84e3-2127779e15f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e8169-652c-4d81-a4cc-2e4f26b34b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define the number of teams per plot and the total number of teams\n",
    "# teams_per_plot = 5\n",
    "# total_teams = seasonal_team_averages.columns.size\n",
    "\n",
    "# # Calculate the number of required plots (6 in this case for 30 teams)\n",
    "# num_plots = total_teams // teams_per_plot\n",
    "\n",
    "# # Create a figure with subplots\n",
    "# fig, axs = plt.subplots(num_plots, 1, figsize=(15, 4 * num_plots), sharex=True)\n",
    "\n",
    "# # Loop over the number of plots, plotting 5 teams at a time\n",
    "# for i in range(num_plots):\n",
    "#     # Get the subset of teams for the current plot\n",
    "#     subset_of_teams = seasonal_team_averages.columns[i*teams_per_plot:(i+1)*teams_per_plot]\n",
    "#     for team in subset_of_teams:\n",
    "#         axs[i].plot(seasonal_team_averages.index, seasonal_team_averages[team], marker='o', label=team)\n",
    "    \n",
    "#     axs[i].set_title(f'Team Performance Comparison {i+1}')\n",
    "#     axs[i].set_ylabel('Avg Field Goal %')\n",
    "#     axs[i].grid(True)\n",
    "#     axs[i].legend()\n",
    "\n",
    "# # Set common X label\n",
    "# plt.xlabel('NBA Season')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50949a05-894b-4bf7-9073-24409a530318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_team_performance( metric):\n",
    "    # Determine if the metric is a percentage (between 0 and 1)\n",
    "    percentage_scale = df[metric].max() <= 1\n",
    "    \n",
    "    # Calculate the average metric for each team per season\n",
    "    seasonal_team_averages = df.groupby(['season', 'team'])[metric].mean().unstack()\n",
    "\n",
    "    # Scale up if the metric is a percentage\n",
    "    if percentage_scale:\n",
    "        seasonal_team_averages *= 100\n",
    "        ylabel = f'Average {metric} (%)'\n",
    "    else:\n",
    "        ylabel = f'Average {metric}'\n",
    "\n",
    "    # Identify the top and bottom 5 performing teams\n",
    "    top_teams = seasonal_team_averages.mean(axis=0).sort_values(ascending=False).head(5).index\n",
    "    bottom_teams = seasonal_team_averages.mean(axis=0).sort_values(ascending=True).head(5).index\n",
    "\n",
    "    # Create subplots for the top and bottom performing teams\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "    # Top 5 performing teams plot\n",
    "    for team in top_teams:\n",
    "        axs[0].plot(seasonal_team_averages.index, seasonal_team_averages.loc[:, team], marker='o', label=team)\n",
    "    axs[0].set_title(f'Top 5 Performing Teams by {metric}')\n",
    "    axs[0].set_ylabel(ylabel)\n",
    "    axs[0].grid(True)\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Bottom 5 performing teams plot\n",
    "    for team in bottom_teams:\n",
    "        axs[1].plot(seasonal_team_averages.index, seasonal_team_averages.loc[:, team], marker='o', label=team)\n",
    "    axs[1].set_title(f'Bottom 5 Performing Teams by {metric}')\n",
    "    axs[1].set_ylabel(ylabel)\n",
    "    axs[1].grid(True)\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Set common X label\n",
    "    plt.xlabel('NBA Season')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671cc75d-fcf6-4e32-a852-1445c9e5dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_team_performance('fg%_total') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da8edf-714a-4e6b-8b04-f819f3d9742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(df[['fg_total', 'fga_total', 'fg%_total', '3p_total', '3pa_total', '3p%_total', 'ft_total', 'fta_total', 'ft%_total', 'total_opp', 'won', 'target']].corr(), annot=True, fmt=\".2f\", cmap='coolwarm', ax=ax)\n",
    "ax.set_title('Correlation Matrix of Selected Metrics with Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b12287-e88f-4a46-94fc-e145485d8997",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab1c1d-b765-4272-aeae-0ada700d3b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the DataFrame specifically for classification\n",
    "df_logisticRegression = df.drop(['won'], axis=1)  # Drop 'won' to avoid data leakage\n",
    "\n",
    "# Selecting numerical features for the model (excluding categorical features and the target)\n",
    "numerical_features = df_logisticRegression.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_features.remove('target')  # Remove the target from the features list\n",
    "\n",
    "# Define features X and target y\n",
    "X = df_logisticRegression[numerical_features]\n",
    "y = df_logisticRegression['target'].astype(int)\n",
    "\n",
    "# Scaling features with MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(multi_class='ovr')  # 'ovr' indicates a one-vs-rest approach for multi-class\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string_S = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "print(\"Started at = \", dt_string_S)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)  # Get probabilities for AUC calculation\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string_E = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "print(\"Ended at = \", dt_string_E)\n",
    "\n",
    "\n",
    "# Calculate Precision, Recall, and F1-Score\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1-Score: {f1 * 100:.2f}%\")\n",
    "\n",
    "# Calculate the AUC Score for multi-class classification\n",
    "# Assuming a one-vs-rest method for multi-class AUC calculation\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "print(f\"AUC Score: {auc_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a3a08c-26c1-4567-93b9-29ca8242c7b7",
   "metadata": {},
   "source": [
    "1. Moderate Classification Accuracy: The precision, recall, and F1-score values are around 51%, indicating a moderate level of accuracy and balance between the sensitivity and positive predictive value of the model.\n",
    "2. Consistent Precision-Recall Balance: The close values of precision and recall suggest that the model is equally balanced in terms of avoiding false positives and false negatives, but improvements are necessary for both.\n",
    "3. Average Discriminative Ability: An AUC score of 0.66 indicates that the model has average ability to discriminate between classes, which is acceptable but suggests there is significant room for improvement in model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424bff5-d858-4f85-96c9-85ff7b5d6f8f",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb65351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "## Selecting features and removing unnecessary columns\n",
    "removed_columns = [\"season\", \"date\", \"won\", \"target\", \"team\", \"team_opp\"]\n",
    "selected_columns = df.columns[~df.columns.isin(removed_columns)]\n",
    "object_columns = df[selected_columns].select_dtypes(include='object').columns\n",
    "\n",
    "## Convert object type columns to integers\n",
    "for column in object_columns:\n",
    "    df[column] = df[column].astype(int)\n",
    "\n",
    "## Scaling features\n",
    "scaler = MinMaxScaler()\n",
    "df[selected_columns] = scaler.fit_transform(df[selected_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7736b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string_S = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "print(\"Started at = \", dt_string_S)\n",
    "\n",
    "# Setting up the SVM model\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True)\n",
    "split = TimeSeriesSplit(n_splits=3)\n",
    "sfs = SequentialFeatureSelector(svm_model, \n",
    "                                n_features_to_select=30, \n",
    "                                direction=\"forward\",\n",
    "                                cv=split,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "# Feature Selection\n",
    "sfs.fit(df[selected_columns], df[\"target\"])\n",
    "predictors = list(selected_columns[sfs.get_support()])\n",
    "print(predictors)\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string_E = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "print(\"Ended at = \", dt_string_E)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181b0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtesting Function with Time Tracking\n",
    "def backtest(data, model, predictors, start=2, step=1):\n",
    "    all_predictions = []\n",
    "    seasons = sorted(data[\"season\"].unique())\n",
    "    overall_start_time = time.time()\n",
    "    \n",
    "    for i in range(start, len(seasons), step):\n",
    "        season_start_time = time.time()\n",
    "        season = seasons[i]\n",
    "        train = data[data[\"season\"] < season]\n",
    "        test = data[data[\"season\"] == season]\n",
    "        \n",
    "        model.fit(train[predictors], train[\"target\"])\n",
    "        preds = model.predict(test[predictors])\n",
    "        preds_proba = model.predict_proba(test[predictors])[:, 1]\n",
    "        \n",
    "        combined = pd.concat([test[\"target\"], pd.Series(preds, index=test.index), pd.Series(preds_proba, index=test.index)], axis=1)\n",
    "        combined.columns = [\"actual\", \"prediction\", \"probabilities\"]\n",
    "        \n",
    "        all_predictions.append(combined)\n",
    "        season_end_time = time.time()\n",
    "        print(f\"Season {season} processing completed in {season_end_time - season_start_time:.2f} seconds.\")\n",
    "    \n",
    "    overall_end_time = time.time()\n",
    "    print(f\"Total backtesting time: {overall_end_time - overall_start_time:.2f} seconds.\")\n",
    "    return pd.concat(all_predictions)\n",
    "\n",
    "predictions = backtest(df, svm_model, predictors)\n",
    "\n",
    "# Remove unpredicted matches\n",
    "predictions = predictions[predictions['actual'] != 2]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40fa88-6f2e-4b65-8d5d-d9850a310593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "print(\"Accuracy Score: \", accuracy_score(predictions[\"actual\"], predictions[\"prediction\"]))\n",
    "print(\"Precision: \", precision_score(predictions[\"actual\"], predictions[\"prediction\"], average='weighted'))\n",
    "print(\"Recall: \", recall_score(predictions[\"actual\"], predictions[\"prediction\"], average='weighted'))\n",
    "print(\"F1-Score: \", f1_score(predictions[\"actual\"], predictions[\"prediction\"], average='weighted'))\n",
    "print(\"AUC Score: \", roc_auc_score(predictions[\"actual\"], predictions[\"probabilities\"], multi_class='ovo', average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8ecfbb-8a13-478b-98ab-eb33d4872bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
