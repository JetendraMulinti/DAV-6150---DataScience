{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3563ad-17b3-4863-ac55-7647d8c71dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01327eab-cd98-42b6-b68d-b017dcae07a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved dataframes_list_season_-2016.pkl as season_-2016.csv\n",
      "Processed and saved dataframes_list_season_-2017.pkl as season_-2017.csv\n",
      "Processed and saved dataframes_list_season_-2018.pkl as season_-2018.csv\n",
      "Processed and saved dataframes_list_season_-2019.pkl as season_-2019.csv\n",
      "Processed and saved dataframes_list_season_-2020.pkl as season_-2020.csv\n",
      "Processed and saved dataframes_list_season_-2021.pkl as season_-2021.csv\n",
      "Processed and saved dataframes_list_season_-2022.pkl as season_-2022.csv\n",
      "Processed and saved dataframes_list_season_-2023.pkl as season_-2023.csv\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import pickle\n",
    "# import os\n",
    "\n",
    "# file_names = [\n",
    "#             'dataframes_list_season_-2016.pkl','dataframes_list_season_-2017.pkl',\n",
    "#              'dataframes_list_season_-2018.pkl', 'dataframes_list_season_-2019.pkl',\n",
    "#               'dataframes_list_season_-2020.pkl', 'dataframes_list_season_-2021.pkl',\n",
    "#               'dataframes_list_season_-2022.pkl','dataframes_list_season_-2023.pkl']\n",
    "\n",
    "\n",
    "# # Directory where the files are located\n",
    "# directory = \"Data/\"\n",
    "\n",
    "# for file_name in file_names:\n",
    "#     try:\n",
    "#         # Construct the full path to the file\n",
    "#         file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "#         # Load the dataframe from the pickle file\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             dataframe = pickle.load(file)\n",
    "\n",
    "#         # Extract the year from the file name\n",
    "#         # The year is located between the last dash and the dot before 'pkl'\n",
    "#         year = file_name.split('_')[-1].split('.')[0]  # Splits the file name and takes the year part\n",
    "\n",
    "#         # Define the CSV file name based on the year extracted\n",
    "#         csv_file_name = f\"season_{year}.csv\"\n",
    "\n",
    "#         # Save the dataframe to CSV in the specified directory\n",
    "#         dataframe.to_csv(os.path.join(directory, csv_file_name))\n",
    "\n",
    "#         print(f\"Processed and saved {file_name} as {csv_file_name}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {file_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26be90aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data from https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2016.csv\n",
      "Processed data from https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2017.csv\n",
      "Processed data from https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2018.csv\n",
      "Processed data from https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2019.csv\n",
      "Processed data from https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2020.csv\n",
      "Processed data from https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2021.csv\n",
      "Processed data from https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2022.csv\n",
      "Processed data from https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2023.csv\n",
      "No of duplicate rows:  0\n",
      "No of duplicate rows after dropping duplicates:  0\n",
      "data shape: (20392, 151)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mp</th>\n",
       "      <th>fg_total</th>\n",
       "      <th>fga_total</th>\n",
       "      <th>fg%_total</th>\n",
       "      <th>3p_total</th>\n",
       "      <th>3pa_total</th>\n",
       "      <th>3p%_total</th>\n",
       "      <th>ft_total</th>\n",
       "      <th>fta_total</th>\n",
       "      <th>ft%_total</th>\n",
       "      <th>orb_total</th>\n",
       "      <th>drb_total</th>\n",
       "      <th>trb_total</th>\n",
       "      <th>ast_total</th>\n",
       "      <th>stl_total</th>\n",
       "      <th>blk_total</th>\n",
       "      <th>tov_total</th>\n",
       "      <th>pf_total</th>\n",
       "      <th>pts_total</th>\n",
       "      <th>ts%_total</th>\n",
       "      <th>efg%_total</th>\n",
       "      <th>3par_total</th>\n",
       "      <th>ftr_total</th>\n",
       "      <th>orb%_total</th>\n",
       "      <th>drb%_total</th>\n",
       "      <th>trb%_total</th>\n",
       "      <th>ast%_total</th>\n",
       "      <th>stl%_total</th>\n",
       "      <th>blk%_total</th>\n",
       "      <th>tov%_total</th>\n",
       "      <th>usg%_total</th>\n",
       "      <th>ortg_total</th>\n",
       "      <th>drtg_total</th>\n",
       "      <th>fg_max</th>\n",
       "      <th>fga_max</th>\n",
       "      <th>fg%_max</th>\n",
       "      <th>3p_max</th>\n",
       "      <th>3pa_max</th>\n",
       "      <th>3p%_max</th>\n",
       "      <th>ft_max</th>\n",
       "      <th>fta_max</th>\n",
       "      <th>ft%_max</th>\n",
       "      <th>orb_max</th>\n",
       "      <th>drb_max</th>\n",
       "      <th>trb_max</th>\n",
       "      <th>ast_max</th>\n",
       "      <th>stl_max</th>\n",
       "      <th>blk_max</th>\n",
       "      <th>tov_max</th>\n",
       "      <th>pf_max</th>\n",
       "      <th>pts_max</th>\n",
       "      <th>+/-_max</th>\n",
       "      <th>ts%_max</th>\n",
       "      <th>efg%_max</th>\n",
       "      <th>3par_max</th>\n",
       "      <th>ftr_max</th>\n",
       "      <th>orb%_max</th>\n",
       "      <th>drb%_max</th>\n",
       "      <th>trb%_max</th>\n",
       "      <th>ast%_max</th>\n",
       "      <th>stl%_max</th>\n",
       "      <th>blk%_max</th>\n",
       "      <th>tov%_max</th>\n",
       "      <th>usg%_max</th>\n",
       "      <th>ortg_max</th>\n",
       "      <th>drtg_max</th>\n",
       "      <th>team</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>total</th>\n",
       "      <th>home</th>\n",
       "      <th>fg_total_opp</th>\n",
       "      <th>fga_total_opp</th>\n",
       "      <th>fg%_total_opp</th>\n",
       "      <th>3p_total_opp</th>\n",
       "      <th>3pa_total_opp</th>\n",
       "      <th>3p%_total_opp</th>\n",
       "      <th>ft_total_opp</th>\n",
       "      <th>fta_total_opp</th>\n",
       "      <th>ft%_total_opp</th>\n",
       "      <th>orb_total_opp</th>\n",
       "      <th>drb_total_opp</th>\n",
       "      <th>trb_total_opp</th>\n",
       "      <th>ast_total_opp</th>\n",
       "      <th>stl_total_opp</th>\n",
       "      <th>blk_total_opp</th>\n",
       "      <th>tov_total_opp</th>\n",
       "      <th>pf_total_opp</th>\n",
       "      <th>pts_total_opp</th>\n",
       "      <th>ts%_total_opp</th>\n",
       "      <th>efg%_total_opp</th>\n",
       "      <th>3par_total_opp</th>\n",
       "      <th>ftr_total_opp</th>\n",
       "      <th>orb%_total_opp</th>\n",
       "      <th>drb%_total_opp</th>\n",
       "      <th>trb%_total_opp</th>\n",
       "      <th>ast%_total_opp</th>\n",
       "      <th>stl%_total_opp</th>\n",
       "      <th>blk%_total_opp</th>\n",
       "      <th>tov%_total_opp</th>\n",
       "      <th>usg%_total_opp</th>\n",
       "      <th>ortg_total_opp</th>\n",
       "      <th>drtg_total_opp</th>\n",
       "      <th>fg_max_opp</th>\n",
       "      <th>fga_max_opp</th>\n",
       "      <th>fg%_max_opp</th>\n",
       "      <th>3p_max_opp</th>\n",
       "      <th>3pa_max_opp</th>\n",
       "      <th>3p%_max_opp</th>\n",
       "      <th>ft_max_opp</th>\n",
       "      <th>fta_max_opp</th>\n",
       "      <th>ft%_max_opp</th>\n",
       "      <th>orb_max_opp</th>\n",
       "      <th>drb_max_opp</th>\n",
       "      <th>trb_max_opp</th>\n",
       "      <th>ast_max_opp</th>\n",
       "      <th>stl_max_opp</th>\n",
       "      <th>blk_max_opp</th>\n",
       "      <th>tov_max_opp</th>\n",
       "      <th>pf_max_opp</th>\n",
       "      <th>pts_max_opp</th>\n",
       "      <th>+/-_max_opp</th>\n",
       "      <th>ts%_max_opp</th>\n",
       "      <th>efg%_max_opp</th>\n",
       "      <th>3par_max_opp</th>\n",
       "      <th>ftr_max_opp</th>\n",
       "      <th>orb%_max_opp</th>\n",
       "      <th>drb%_max_opp</th>\n",
       "      <th>trb%_max_opp</th>\n",
       "      <th>ast%_max_opp</th>\n",
       "      <th>stl%_max_opp</th>\n",
       "      <th>blk%_max_opp</th>\n",
       "      <th>tov%_max_opp</th>\n",
       "      <th>usg%_max_opp</th>\n",
       "      <th>ortg_max_opp</th>\n",
       "      <th>drtg_max_opp</th>\n",
       "      <th>team_opp</th>\n",
       "      <th>1_opp</th>\n",
       "      <th>2_opp</th>\n",
       "      <th>3_opp</th>\n",
       "      <th>4_opp</th>\n",
       "      <th>total_opp</th>\n",
       "      <th>home_opp</th>\n",
       "      <th>date</th>\n",
       "      <th>season</th>\n",
       "      <th>won</th>\n",
       "      <th>4OT</th>\n",
       "      <th>4OT_opp</th>\n",
       "      <th>index_opp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>240.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.404</td>\n",
       "      <td>9.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.588</td>\n",
       "      <td>11.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.181</td>\n",
       "      <td>21.6</td>\n",
       "      <td>84.8</td>\n",
       "      <td>51.5</td>\n",
       "      <td>68.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.5</td>\n",
       "      <td>97.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.571</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.714</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>10.8</td>\n",
       "      <td>41.9</td>\n",
       "      <td>23.8</td>\n",
       "      <td>31.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>18.5</td>\n",
       "      <td>30.4</td>\n",
       "      <td>29.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>CLE</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.425</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.368</td>\n",
       "      <td>16.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.696</td>\n",
       "      <td>7.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.264</td>\n",
       "      <td>15.2</td>\n",
       "      <td>78.4</td>\n",
       "      <td>48.5</td>\n",
       "      <td>35.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.4</td>\n",
       "      <td>11.8</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.5</td>\n",
       "      <td>95.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.750</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.556</td>\n",
       "      <td>1.333</td>\n",
       "      <td>12.2</td>\n",
       "      <td>38.5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>53.2</td>\n",
       "      <td>34.6</td>\n",
       "      <td>162.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-10-27</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>240.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.425</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.368</td>\n",
       "      <td>16.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.696</td>\n",
       "      <td>7.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.264</td>\n",
       "      <td>15.2</td>\n",
       "      <td>78.4</td>\n",
       "      <td>48.5</td>\n",
       "      <td>35.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.4</td>\n",
       "      <td>11.8</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.5</td>\n",
       "      <td>95.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.750</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.556</td>\n",
       "      <td>1.333</td>\n",
       "      <td>12.2</td>\n",
       "      <td>38.5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>53.2</td>\n",
       "      <td>34.6</td>\n",
       "      <td>162.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.404</td>\n",
       "      <td>9.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.588</td>\n",
       "      <td>11.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.181</td>\n",
       "      <td>21.6</td>\n",
       "      <td>84.8</td>\n",
       "      <td>51.5</td>\n",
       "      <td>68.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.5</td>\n",
       "      <td>97.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.571</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.714</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>10.8</td>\n",
       "      <td>41.9</td>\n",
       "      <td>23.8</td>\n",
       "      <td>31.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>18.5</td>\n",
       "      <td>30.4</td>\n",
       "      <td>29.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>CLE</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-10-27</td>\n",
       "      <td>2015</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mp  fg_total  fga_total  fg%_total  3p_total  3pa_total  3p%_total  \\\n",
       "0  240.0      38.0       94.0      0.404       9.0       29.0      0.310   \n",
       "1  240.0      37.0       87.0      0.425       7.0       19.0      0.368   \n",
       "\n",
       "   ft_total  fta_total  ft%_total  orb_total  drb_total  trb_total  ast_total  \\\n",
       "0      10.0       17.0      0.588       11.0       39.0       50.0       26.0   \n",
       "1      16.0       23.0      0.696        7.0       40.0       47.0       13.0   \n",
       "\n",
       "   stl_total  blk_total  tov_total  pf_total  pts_total  ts%_total  \\\n",
       "0        5.0        7.0       10.0      21.0       95.0      0.468   \n",
       "1        6.0       10.0       13.0      22.0       97.0      0.499   \n",
       "\n",
       "   efg%_total  3par_total  ftr_total  orb%_total  drb%_total  trb%_total  \\\n",
       "0       0.452       0.309      0.181        21.6        84.8        51.5   \n",
       "1       0.466       0.218      0.264        15.2        78.4        48.5   \n",
       "\n",
       "   ast%_total  stl%_total  blk%_total  tov%_total  usg%_total  ortg_total  \\\n",
       "0        68.4         5.0        10.3         9.0       100.0        95.5   \n",
       "1        35.1         6.0        15.4        11.8       100.0        97.5   \n",
       "\n",
       "   drtg_total  fg_max  fga_max  fg%_max  3p_max  3pa_max  3p%_max  ft_max  \\\n",
       "0        97.5    12.0     22.0    0.571     3.0      7.0     0.50     3.0   \n",
       "1        95.5     8.0     22.0    0.750     3.0      5.0     0.75     5.0   \n",
       "\n",
       "   fta_max  ft%_max  orb_max  drb_max  trb_max  ast_max  stl_max  blk_max  \\\n",
       "0      4.0      1.0      4.0     10.0     12.0      7.0      1.0      2.0   \n",
       "1      5.0      1.0      2.0      8.0     10.0      5.0      2.0      6.0   \n",
       "\n",
       "   tov_max  pf_max  pts_max  +/-_max  ts%_max  efg%_max  3par_max  ftr_max  \\\n",
       "0      3.0     4.0     25.0      9.0    0.714     0.714     1.000    2.000   \n",
       "1      4.0     6.0     19.0      9.0    0.820     0.875     0.556    1.333   \n",
       "\n",
       "   orb%_max  drb%_max  trb%_max  ast%_max  stl%_max  blk%_max  tov%_max  \\\n",
       "0      10.8      41.9      23.8      31.2       2.8      18.5      30.4   \n",
       "1      12.2      38.5      26.0      30.3       2.8      14.0      53.2   \n",
       "\n",
       "   usg%_max  ortg_max  drtg_max team   1   2   3   4  total  home  \\\n",
       "0      29.0     138.0     105.0  CLE  17  23  28  27     95     0   \n",
       "1      34.6     162.0     104.0  CHI  26  20  25  26     97     1   \n",
       "\n",
       "   fg_total_opp  fga_total_opp  fg%_total_opp  3p_total_opp  3pa_total_opp  \\\n",
       "0          37.0           87.0          0.425           7.0           19.0   \n",
       "1          38.0           94.0          0.404           9.0           29.0   \n",
       "\n",
       "   3p%_total_opp  ft_total_opp  fta_total_opp  ft%_total_opp  orb_total_opp  \\\n",
       "0          0.368          16.0           23.0          0.696            7.0   \n",
       "1          0.310          10.0           17.0          0.588           11.0   \n",
       "\n",
       "   drb_total_opp  trb_total_opp  ast_total_opp  stl_total_opp  blk_total_opp  \\\n",
       "0           40.0           47.0           13.0            6.0           10.0   \n",
       "1           39.0           50.0           26.0            5.0            7.0   \n",
       "\n",
       "   tov_total_opp  pf_total_opp  pts_total_opp  ts%_total_opp  efg%_total_opp  \\\n",
       "0           13.0          22.0           97.0          0.499           0.466   \n",
       "1           10.0          21.0           95.0          0.468           0.452   \n",
       "\n",
       "   3par_total_opp  ftr_total_opp  orb%_total_opp  drb%_total_opp  \\\n",
       "0           0.218          0.264            15.2            78.4   \n",
       "1           0.309          0.181            21.6            84.8   \n",
       "\n",
       "   trb%_total_opp  ast%_total_opp  stl%_total_opp  blk%_total_opp  \\\n",
       "0            48.5            35.1             6.0            15.4   \n",
       "1            51.5            68.4             5.0            10.3   \n",
       "\n",
       "   tov%_total_opp  usg%_total_opp  ortg_total_opp  drtg_total_opp  fg_max_opp  \\\n",
       "0            11.8           100.0            97.5            95.5         8.0   \n",
       "1             9.0           100.0            95.5            97.5        12.0   \n",
       "\n",
       "   fga_max_opp  fg%_max_opp  3p_max_opp  3pa_max_opp  3p%_max_opp  ft_max_opp  \\\n",
       "0         22.0        0.750         3.0          5.0         0.75         5.0   \n",
       "1         22.0        0.571         3.0          7.0         0.50         3.0   \n",
       "\n",
       "   fta_max_opp  ft%_max_opp  orb_max_opp  drb_max_opp  trb_max_opp  \\\n",
       "0          5.0          1.0          2.0          8.0         10.0   \n",
       "1          4.0          1.0          4.0         10.0         12.0   \n",
       "\n",
       "   ast_max_opp  stl_max_opp  blk_max_opp  tov_max_opp  pf_max_opp  \\\n",
       "0          5.0          2.0          6.0          4.0         6.0   \n",
       "1          7.0          1.0          2.0          3.0         4.0   \n",
       "\n",
       "   pts_max_opp  +/-_max_opp  ts%_max_opp  efg%_max_opp  3par_max_opp  \\\n",
       "0         19.0          9.0        0.820         0.875         0.556   \n",
       "1         25.0          9.0        0.714         0.714         1.000   \n",
       "\n",
       "   ftr_max_opp  orb%_max_opp  drb%_max_opp  trb%_max_opp  ast%_max_opp  \\\n",
       "0        1.333          12.2          38.5          26.0          30.3   \n",
       "1        2.000          10.8          41.9          23.8          31.2   \n",
       "\n",
       "   stl%_max_opp  blk%_max_opp  tov%_max_opp  usg%_max_opp  ortg_max_opp  \\\n",
       "0           2.8          14.0          53.2          34.6         162.0   \n",
       "1           2.8          18.5          30.4          29.0         138.0   \n",
       "\n",
       "   drtg_max_opp team_opp  1_opp  2_opp  3_opp  4_opp  total_opp  home_opp  \\\n",
       "0         104.0      CHI     26     20     25     26         97         1   \n",
       "1         105.0      CLE     17     23     28     27         95         0   \n",
       "\n",
       "        date  season    won  4OT  4OT_opp  index_opp  \n",
       "0 2015-10-27    2015  False  NaN      NaN        NaN  \n",
       "1 2015-10-27    2015   True  NaN      NaN        NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of GitHub raw URLs\n",
    "urls = [\n",
    "    ## 'https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2016.csv',\n",
    "    ## 'https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2017.csv',\n",
    "    ## 'https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2018.csv',\n",
    "    ## 'https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2019.csv',\n",
    "    'https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2020.csv',\n",
    "    'https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2021.csv',\n",
    "    'https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2022.csv',\n",
    "    'https://raw.githubusercontent.com/JetendraMulinti/DAV-6150---DataScience/main/FinalProject-NBA_Prediction/Data/season_-2023.csv'\n",
    "]\n",
    "\n",
    "all_cleaned_dataframes = []\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Load the dataframe from pickle data obtained from URL\n",
    "        dataframe = pd.read_csv(url)\n",
    "        \n",
    "        # Filter out columns that start with 'Unnamed:'\n",
    "        dataframe = dataframe.loc[:, ~dataframe.columns.str.startswith('Unnamed:')]\n",
    "\n",
    "        # Drop all columns that are entirely NA\n",
    "        dataframe = dataframe.dropna(axis=1, how='all')\n",
    "\n",
    "        # Add the cleaned dataframe to the list\n",
    "        all_cleaned_dataframes.append(dataframe)\n",
    "        \n",
    "        print(f\"Processed data from {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "df = pd.concat(all_cleaned_dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "### delete some more columns\n",
    "columns_to_delete = ['OT', 'OT_opp', '2OT', '3OT', '2OT_opp', '3OT_opp',\n",
    "                     ## '4OT', '4OT_opp',\n",
    "                    'mp_total_opp','bpm_max','bpm_max_opp']\n",
    "\n",
    "# Drop the specified columns from the dataframe\n",
    "df.drop(columns=columns_to_delete, inplace=True)\n",
    "\n",
    "\n",
    "print(\"No of duplicate rows: \",df.duplicated().sum())\n",
    "\n",
    "### Drop duplicates\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(\"No of duplicate rows after dropping duplicates: \",df.duplicated().sum())\n",
    "\n",
    "#### rename columns\n",
    "df.rename(columns = {'mp_total':'mp'}, inplace=True)\n",
    "\n",
    "#### Creating Season column\n",
    "df['date'] = pd.to_datetime(df['date'])  # Convert 'date' column to datetime if it's not already\n",
    "\n",
    "# Function to determine the season year based on the month\n",
    "def get_season_year(row):\n",
    "    if row['date'].month >= 10:\n",
    "        return row['date'].year\n",
    "    else:\n",
    "        return row['date'].year - 1\n",
    "\n",
    "# Apply the function to create a new 'season' column\n",
    "df['season'] = df.apply(get_season_year, axis=1)\n",
    "\n",
    "\n",
    "print(\"data shape:\", df.shape)\n",
    "\n",
    "columns_format = list(df.columns)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "994cb00a-72d5-4013-bd3f-98d9655490aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://github.com/JetendraMulinti/DAV-6150---DataScience/blob/main/FinalProject-NBA_Prediction/Data/dataframes_list_season_-2020.pkl'\n",
    "\n",
    "requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083779e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Abbrivate the Team names\n",
    "team_df = pd.read_excel(\"Data/\"+'Team_full-forms.xlsx')\n",
    "team_df['team'] = team_df['team'].str.strip()\n",
    "team_df['team1'] = team_df['team1'].str.strip()\n",
    "\n",
    "\n",
    "##### Merge and delete the columns\n",
    "df = pd.merge(team_df, df, on = ['team'], how='inner')\n",
    "del df['team']\n",
    "df.rename(columns = {'team1':'team'}, inplace=True)\n",
    "\n",
    "team_df.rename(columns = {'team':'team_opp'}, inplace=True)\n",
    "df = pd.merge(team_df, df, on = ['team_opp'], how='inner')\n",
    "del df['team_opp']\n",
    "df.rename(columns = {'team1':'team_opp'}, inplace=True)\n",
    "\n",
    "print(\"data shape:\", df.shape)\n",
    "\n",
    "df = df[columns_format]\n",
    "\n",
    "## ordering with date\n",
    "df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "df = df.sort_values(by = ['date'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4530f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['season'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc69ab03",
   "metadata": {},
   "source": [
    "1. We will create a target column (needs to be created on team level) -> represents the next game outcome. (Won column indicates the current match, target column indicates next match)\n",
    "\n",
    "2. Replace the Null values in Target column with “2”, False (Loss) = 0, True (Won) = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf0b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target(team):\n",
    "    team['target'] = team['won'].shift(-1)\n",
    "    return team\n",
    "\n",
    "df = df.groupby(\"team\", group_keys=False).apply(add_target)\n",
    "\n",
    "## Preprocessing Target column (Null = 2, True = 1, False = 0)\n",
    "\n",
    "df['target'][pd.isnull(df['target'])] = 2\n",
    "df['target'] = df['target'].astype(int, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68386cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### checking the data is balance / Imbalanced\n",
    "\n",
    "df['won'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded602c",
   "metadata": {},
   "source": [
    "Checking Null values and dropping columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07971fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checking null values\n",
    "\n",
    "null_columns = df.isnull().sum()\n",
    "null_columns[null_columns > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### delete some more columns\n",
    "more_columns_to_delete = ['index_opp']\n",
    "\n",
    "# Drop the specified columns from the dataframe\n",
    "df.drop(columns=more_columns_to_delete, inplace=True)\n",
    "\n",
    "## as we have only 1 null row (match) we will drop it\n",
    "df = df.dropna()\n",
    "\n",
    "null_columns = df.isnull().sum()\n",
    "null_columns[null_columns > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c70da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## re-ordering on date\n",
    "\n",
    "## ordering with date\n",
    "df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "df = df.sort_values(by = ['date'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "print(\"data shape:\", df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b7d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for key metrics\n",
    "key_metrics = ['fg_total', 'fga_total', 'fg%_total', '3p_total', '3pa_total', '3p%_total', 'ft_total',\n",
    "               'fta_total', 'ft%_total', 'total_opp']\n",
    "\n",
    "# Selecting the key metrics and generating descriptive statistics\n",
    "key_stats_summary = df[key_metrics].describe()\n",
    "\n",
    "# Display the descriptive statistics for key metrics\n",
    "key_stats_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800bcae8",
   "metadata": {},
   "source": [
    "1. Field Goals Made and Attempted (fg_total, fga_total): Teams make an average of 40 field goals per game from 87 attempts, translating to an average field goal percentage of 46.2%.\n",
    "2. Three-Point Shots (3p_total, 3pa_total, 3p%_total): On average, teams successfully make 11 three-point shots per game from 31 attempts, achieving a three-point shooting percentage of 35.7%.\n",
    "3. Free Throws (ft_total, fta_total, ft%_total): Teams typically make 17 free throws per game from 23 attempts, with an average success rate of 77.2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f6625",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "\n",
    "# Plotting field goals, three-point shots, and free throws\n",
    "sns.histplot(df['fg_total'], bins=30, kde=True, ax=axes[0, 0]).set_title('Field Goals Made')\n",
    "sns.histplot(df['3p_total'], bins=30, kde=True, ax=axes[0, 1]).set_title('Three-Points Made')\n",
    "sns.histplot(df['ft_total'], bins=30, kde=True, ax=axes[0, 2]).set_title('Free Throws Made')\n",
    "\n",
    "# Plotting percentages for field goals, three-point shots, and free throws\n",
    "sns.histplot(df['fg%_total'], bins=30, kde=True, ax=axes[1, 0]).set_title('Field Goal Percentage')\n",
    "sns.histplot(df['3p%_total'], bins=30, kde=True, ax=axes[1, 1]).set_title('Three-Point Percentage')\n",
    "sns.histplot(df['ft%_total'], bins=30, kde=True, ax=axes[1, 2]).set_title('Free Throw Percentage')\n",
    "\n",
    "# Plotting games per season and distributions of win and next game outcomes\n",
    "sns.histplot(df['season'], bins=len(df['season'].unique()), kde=False, ax=axes[2, 0]).set_title('Games per Season')\n",
    "sns.countplot(x='won', data=df, ax=axes[2, 1]).set_title('Win Distribution')\n",
    "sns.countplot(x='target', data=df, ax=axes[2, 2]).set_title('Next Game Outcome Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2badae-c404-4518-aef1-73e200e4a40f",
   "metadata": {},
   "source": [
    "1. Field Goals Made and Three-Points Made distributions center around a common range, indicating a pattern in scoring strategies across games.\n",
    "2. Percentage metrics for Field Goals, Three-Points, and Free Throws exhibit a normal distribution, reflecting a standard level of efficiency across matches.\n",
    "3. The Games per Season distribution shows consistency in the number of games played, which supports analyses over multiple seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783381d1-d413-45c4-b1db-3657936504da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_trend(column):\n",
    "    # Check if the column data looks like percentages (values between 0 and 1)\n",
    "    if df[column].max() <= 1:\n",
    "        # If so, convert to percentage by multiplying by 100\n",
    "        seasonal_averages = df.groupby('season')[column].mean() * 100\n",
    "        ylabel = f'Average {column} (%)'\n",
    "    else:\n",
    "        # Otherwise, use the values as is\n",
    "        seasonal_averages = df.groupby('season')[column].mean()\n",
    "        ylabel = f'Average {column}'\n",
    "    \n",
    "    # Plotting the time series\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    seasonal_averages.plot(kind='line', marker='o')\n",
    "    plt.title(f'Average {column} by NBA Season')\n",
    "    plt.xlabel('NBA Season')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    plt.xticks(ticks=seasonal_averages.index, labels=seasonal_averages.index)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed311a65-ea3e-4fa2-bd05-bb2dda415118",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_trend('fg%_total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf10d37-ee30-4c57-84e3-2127779e15f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e8169-652c-4d81-a4cc-2e4f26b34b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define the number of teams per plot and the total number of teams\n",
    "# teams_per_plot = 5\n",
    "# total_teams = seasonal_team_averages.columns.size\n",
    "\n",
    "# # Calculate the number of required plots (6 in this case for 30 teams)\n",
    "# num_plots = total_teams // teams_per_plot\n",
    "\n",
    "# # Create a figure with subplots\n",
    "# fig, axs = plt.subplots(num_plots, 1, figsize=(15, 4 * num_plots), sharex=True)\n",
    "\n",
    "# # Loop over the number of plots, plotting 5 teams at a time\n",
    "# for i in range(num_plots):\n",
    "#     # Get the subset of teams for the current plot\n",
    "#     subset_of_teams = seasonal_team_averages.columns[i*teams_per_plot:(i+1)*teams_per_plot]\n",
    "#     for team in subset_of_teams:\n",
    "#         axs[i].plot(seasonal_team_averages.index, seasonal_team_averages[team], marker='o', label=team)\n",
    "    \n",
    "#     axs[i].set_title(f'Team Performance Comparison {i+1}')\n",
    "#     axs[i].set_ylabel('Avg Field Goal %')\n",
    "#     axs[i].grid(True)\n",
    "#     axs[i].legend()\n",
    "\n",
    "# # Set common X label\n",
    "# plt.xlabel('NBA Season')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50949a05-894b-4bf7-9073-24409a530318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_team_performance( metric):\n",
    "    # Determine if the metric is a percentage (between 0 and 1)\n",
    "    percentage_scale = df[metric].max() <= 1\n",
    "    \n",
    "    # Calculate the average metric for each team per season\n",
    "    seasonal_team_averages = df.groupby(['season', 'team'])[metric].mean().unstack()\n",
    "\n",
    "    # Scale up if the metric is a percentage\n",
    "    if percentage_scale:\n",
    "        seasonal_team_averages *= 100\n",
    "        ylabel = f'Average {metric} (%)'\n",
    "    else:\n",
    "        ylabel = f'Average {metric}'\n",
    "\n",
    "    # Identify the top and bottom 5 performing teams\n",
    "    top_teams = seasonal_team_averages.mean(axis=0).sort_values(ascending=False).head(5).index\n",
    "    bottom_teams = seasonal_team_averages.mean(axis=0).sort_values(ascending=True).head(5).index\n",
    "\n",
    "    # Create subplots for the top and bottom performing teams\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "    # Top 5 performing teams plot\n",
    "    for team in top_teams:\n",
    "        axs[0].plot(seasonal_team_averages.index, seasonal_team_averages.loc[:, team], marker='o', label=team)\n",
    "    axs[0].set_title(f'Top 5 Performing Teams by {metric}')\n",
    "    axs[0].set_ylabel(ylabel)\n",
    "    axs[0].grid(True)\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Bottom 5 performing teams plot\n",
    "    for team in bottom_teams:\n",
    "        axs[1].plot(seasonal_team_averages.index, seasonal_team_averages.loc[:, team], marker='o', label=team)\n",
    "    axs[1].set_title(f'Bottom 5 Performing Teams by {metric}')\n",
    "    axs[1].set_ylabel(ylabel)\n",
    "    axs[1].grid(True)\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Set common X label\n",
    "    plt.xlabel('NBA Season')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671cc75d-fcf6-4e32-a852-1445c9e5dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_team_performance('fg%_total') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da8edf-714a-4e6b-8b04-f819f3d9742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(df[['fg_total', 'fga_total', 'fg%_total', '3p_total', '3pa_total', '3p%_total', 'ft_total', 'fta_total', 'ft%_total', 'total_opp', 'won', 'target']].corr(), annot=True, fmt=\".2f\", cmap='coolwarm', ax=ax)\n",
    "ax.set_title('Correlation Matrix of Selected Metrics with Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b12287-e88f-4a46-94fc-e145485d8997",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab1c1d-b765-4272-aeae-0ada700d3b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the DataFrame specifically for classification\n",
    "df_logisticRegression = df.drop(['won'], axis=1)  # Drop 'won' to avoid data leakage\n",
    "\n",
    "# Selecting numerical features for the model (excluding categorical features and the target)\n",
    "numerical_features = df_logisticRegression.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_features.remove('target')  # Remove the target from the features list\n",
    "\n",
    "# Define features X and target y\n",
    "X = df_logisticRegression[numerical_features]\n",
    "y = df_logisticRegression['target'].astype(int)\n",
    "\n",
    "# Scaling features with MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(multi_class='ovr')  # 'ovr' indicates a one-vs-rest approach for multi-class\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string_S = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "print(\"Started at = \", dt_string_S)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)  # Get probabilities for AUC calculation\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string_E = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "print(\"Ended at = \", dt_string_E)\n",
    "\n",
    "\n",
    "# Calculate Precision, Recall, and F1-Score\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1-Score: {f1 * 100:.2f}%\")\n",
    "\n",
    "# Calculate the AUC Score for multi-class classification\n",
    "# Assuming a one-vs-rest method for multi-class AUC calculation\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "print(f\"AUC Score: {auc_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a3a08c-26c1-4567-93b9-29ca8242c7b7",
   "metadata": {},
   "source": [
    "1. Moderate Classification Accuracy: The precision, recall, and F1-score values are around 51%, indicating a moderate level of accuracy and balance between the sensitivity and positive predictive value of the model.\n",
    "2. Consistent Precision-Recall Balance: The close values of precision and recall suggest that the model is equally balanced in terms of avoiding false positives and false negatives, but improvements are necessary for both.\n",
    "3. Average Discriminative Ability: An AUC score of 0.66 indicates that the model has average ability to discriminate between classes, which is acceptable but suggests there is significant room for improvement in model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424bff5-d858-4f85-96c9-85ff7b5d6f8f",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb65351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "## Selecting features and removing unnecessary columns\n",
    "removed_columns = [\"season\", \"date\", \"won\", \"target\", \"team\", \"team_opp\"]\n",
    "selected_columns = df.columns[~df.columns.isin(removed_columns)]\n",
    "object_columns = df[selected_columns].select_dtypes(include='object').columns\n",
    "\n",
    "## Convert object type columns to integers\n",
    "for column in object_columns:\n",
    "    df[column] = df[column].astype(int)\n",
    "\n",
    "## Scaling features\n",
    "scaler = MinMaxScaler()\n",
    "df[selected_columns] = scaler.fit_transform(df[selected_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7736b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string_S = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "print(\"Started at = \", dt_string_S)\n",
    "\n",
    "# Setting up the SVM model\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True)\n",
    "split = TimeSeriesSplit(n_splits=3)\n",
    "sfs = SequentialFeatureSelector(svm_model, \n",
    "                                n_features_to_select=30, \n",
    "                                direction=\"forward\",\n",
    "                                cv=split,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "# Feature Selection\n",
    "sfs.fit(df[selected_columns], df[\"target\"])\n",
    "predictors = list(selected_columns[sfs.get_support()])\n",
    "print(predictors)\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string_E = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "print(\"Ended at = \", dt_string_E)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181b0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtesting Function with Time Tracking\n",
    "def backtest(data, model, predictors, start=2, step=1):\n",
    "    all_predictions = []\n",
    "    seasons = sorted(data[\"season\"].unique())\n",
    "    overall_start_time = time.time()\n",
    "    \n",
    "    for i in range(start, len(seasons), step):\n",
    "        season_start_time = time.time()\n",
    "        season = seasons[i]\n",
    "        train = data[data[\"season\"] < season]\n",
    "        test = data[data[\"season\"] == season]\n",
    "        \n",
    "        model.fit(train[predictors], train[\"target\"])\n",
    "        preds = model.predict(test[predictors])\n",
    "        preds_proba = model.predict_proba(test[predictors])[:, 1]\n",
    "        \n",
    "        combined = pd.concat([test[\"target\"], pd.Series(preds, index=test.index), pd.Series(preds_proba, index=test.index)], axis=1)\n",
    "        combined.columns = [\"actual\", \"prediction\", \"probabilities\"]\n",
    "        \n",
    "        all_predictions.append(combined)\n",
    "        season_end_time = time.time()\n",
    "        print(f\"Season {season} processing completed in {season_end_time - season_start_time:.2f} seconds.\")\n",
    "    \n",
    "    overall_end_time = time.time()\n",
    "    print(f\"Total backtesting time: {overall_end_time - overall_start_time:.2f} seconds.\")\n",
    "    return pd.concat(all_predictions)\n",
    "\n",
    "predictions = backtest(df, svm_model, predictors)\n",
    "\n",
    "# Remove unpredicted matches\n",
    "predictions = predictions[predictions['actual'] != 2]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40fa88-6f2e-4b65-8d5d-d9850a310593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "print(\"Accuracy Score: \", accuracy_score(predictions[\"actual\"], predictions[\"prediction\"]))\n",
    "print(\"Precision: \", precision_score(predictions[\"actual\"], predictions[\"prediction\"], average='weighted'))\n",
    "print(\"Recall: \", recall_score(predictions[\"actual\"], predictions[\"prediction\"], average='weighted'))\n",
    "print(\"F1-Score: \", f1_score(predictions[\"actual\"], predictions[\"prediction\"], average='weighted'))\n",
    "print(\"AUC Score: \", roc_auc_score(predictions[\"actual\"], predictions[\"probabilities\"], multi_class='ovo', average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8ecfbb-8a13-478b-98ab-eb33d4872bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
